<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  
  
  <title>Hexo</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
  <meta name="description" content="CV Algorithm Trick IISummary on RegularizationHow to solve overfitting？ Data: Data augmentation: complexity (color shifting, transform (affine, perspective)，cutmix,  cutout,  mixup, mosaic, random era">
<meta property="og:type" content="article">
<meta property="og:title" content="Hexo">
<meta property="og:url" content="http://example.com/2021/04/29/CV/Algorithm-Trick/CV%20Algorithm%20Trick%20II/index.html">
<meta property="og:site_name" content="Hexo">
<meta property="og:description" content="CV Algorithm Trick IISummary on RegularizationHow to solve overfitting？ Data: Data augmentation: complexity (color shifting, transform (affine, perspective)，cutmix,  cutout,  mixup, mosaic, random era">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="c:/Users/user/AppData/Roaming/Typora/typora-user-images/image-20210224143108958.png">
<meta property="og:image" content="c:/Users/user/AppData/Roaming/Typora/typora-user-images/image-20210224143131588.png">
<meta property="og:image" content="c:/Users/user/AppData/Roaming/Typora/typora-user-images/image-20210224143140588.png">
<meta property="og:image" content="c:/Users/user/AppData/Roaming/Typora/typora-user-images/image-20210224144958353.png">
<meta property="og:image" content="c:/Users/user/AppData/Roaming/Typora/typora-user-images/image-20210224152438117.png">
<meta property="og:image" content="c:/Users/user/AppData/Roaming/Typora/typora-user-images/image-20210224152448268.png">
<meta property="og:image" content="f:/blog/CV/Algorithm%20Trick/pics/smooth-l1.png">
<meta property="og:image" content="f:/blog/CV/Algorithm%20Trick/pics/iou-problem.png">
<meta property="og:image" content="f:/blog/CV/Algorithm%20Trick/pics/iou-problem2.png">
<meta property="og:image" content="f:/blog/CV/Algorithm%20Trick/pics/giou-formula.png">
<meta property="og:image" content="f:/blog/CV/Algorithm%20Trick/pics/Giou-loss.png">
<meta property="og:image" content="f:/blog/CV/Algorithm%20Trick/pics/giou-problem.png">
<meta property="article:published_time" content="2021-04-29T03:06:19.683Z">
<meta property="article:modified_time" content="2021-03-02T08:56:31.403Z">
<meta property="article:author" content="Cheng Zixu">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="c:/Users/user/AppData/Roaming/Typora/typora-user-images/image-20210224143108958.png">
  
    <link rel="alternate" href="/atom.xml" title="Hexo" type="application/atom+xml">
  
  
    <link rel="shortcut icon" href="/favicon.png">
  
  
    
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/typeface-source-code-pro@0.0.71/index.min.css">

  
  
<link rel="stylesheet" href="/css/style.css">

  
    
<link rel="stylesheet" href="/fancybox/jquery.fancybox.min.css">

  
<meta name="generator" content="Hexo 5.4.0"></head>

<body>
  <div id="container">
    <div id="wrap">
      <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <div id="header-title" class="inner">
      <h1 id="logo-wrap">
        <a href="/" id="logo">Hexo</a>
      </h1>
      
    </div>
    <div id="header-inner" class="inner">
      <nav id="main-nav">
        <a id="main-nav-toggle" class="nav-icon"></a>
        
          <a class="main-nav-link" href="/">Home</a>
        
          <a class="main-nav-link" href="/archives">Archives</a>
        
      </nav>
      <nav id="sub-nav">
        
          <a id="nav-rss-link" class="nav-icon" href="/atom.xml" title="RSS Feed"></a>
        
        <a id="nav-search-btn" class="nav-icon" title="Suche"></a>
      </nav>
      <div id="search-form-wrap">
        <form action="//google.com/search" method="get" accept-charset="UTF-8" class="search-form"><input type="search" name="q" class="search-form-input" placeholder="Suche"><button type="submit" class="search-form-submit">&#xF002;</button><input type="hidden" name="sitesearch" value="http://example.com"></form>
      </div>
    </div>
  </div>
</header>

      <div class="outer">
        <section id="main"><article id="post-CV/Algorithm-Trick/CV Algorithm Trick II" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2021/04/29/CV/Algorithm-Trick/CV%20Algorithm%20Trick%20II/" class="article-date">
  <time class="dt-published" datetime="2021-04-29T03:06:19.683Z" itemprop="datePublished">2021-04-29</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <h1 id="CV-Algorithm-Trick-II"><a href="#CV-Algorithm-Trick-II" class="headerlink" title="CV Algorithm Trick II"></a>CV Algorithm Trick II</h1><h2 id="Summary-on-Regularization"><a href="#Summary-on-Regularization" class="headerlink" title="Summary on Regularization"></a>Summary on Regularization</h2><p>How to solve overfitting？</p>
<h3 id="Data"><a href="#Data" class="headerlink" title="Data:"></a>Data:</h3><p> Data augmentation: complexity (color shifting, transform (affine, perspective)，cutmix,  cutout,  mixup, mosaic, random erasing..</p>
<p> Increase quantity: get more data /more balanced</p>
<h3 id="Label"><a href="#Label" class="headerlink" title="Label"></a>Label</h3><p> Smooth labeling </p>
<p> Disturb label</p>
<h3 id="Training"><a href="#Training" class="headerlink" title="Training"></a>Training</h3><p> Early stop</p>
<p> Adversarial training</p>
<h3 id="Network-Task"><a href="#Network-Task" class="headerlink" title="Network/Task:"></a>Network/Task:</h3><p> Simplify network structure </p>
<p>Multi-Tasks  </p>
<h3 id="Regularization"><a href="#Regularization" class="headerlink" title="Regularization:"></a>Regularization:</h3><p>L1/12 regularization </p>
<p>将模型轻量化的方法有什么</p>
<h2 id="I-Mathematical-Operation"><a href="#I-Mathematical-Operation" class="headerlink" title="I. Mathematical Operation"></a>I. Mathematical Operation</h2><h3 id="1-Winograd-2015-Lavin"><a href="#1-Winograd-2015-Lavin" class="headerlink" title="1. Winograd [2015, Lavin]"></a>1. Winograd [2015, Lavin]</h3><p>​    Winograd 最早是在1980年在数字信号处理领域提出了，用于处理一维的信号，2015年被提出可应用扩展在二维图像上。这个算法的主要思想就是替换操作：把一个复杂的运算用多个简单运算代替，如用多个加法代替乘法。</p>
<h1 id="模型压缩的方法"><a href="#模型压缩的方法" class="headerlink" title="模型压缩的方法"></a>模型压缩的方法</h1><h2 id="1-未训练时的压缩"><a href="#1-未训练时的压缩" class="headerlink" title="1.未训练时的压缩"></a>1.未训练时的压缩</h2><p>​    修改为轻量级的backbone，如MobileNet、ShuffleNet。对比模型训练的acc和loss是否维持在可接受的水平。</p>
<h2 id="2-训练好之后的压缩"><a href="#2-训练好之后的压缩" class="headerlink" title="2. 训练好之后的压缩"></a>2. 训练好之后的压缩</h2><h4 id="a-模型训练完成之后的裁剪"><a href="#a-模型训练完成之后的裁剪" class="headerlink" title="a. 模型训练完成之后的裁剪"></a>a. 模型训练完成之后的裁剪</h4><p>​    本质上是寻找某种策略，在减少某些weight的同时，尽量保持acc在可接受水平。</p>
<p>​    可以置零的项：前向计算过程中的任意值，如weight、bias、feature map‘s pixel。</p>
<p>​    如何置零？将哪些值置零？可否总结成规则策略？</p>
<p>​    如何观察acc不变？可以做消融实验。消融实验的记录如下表格所示，没有打钩√的表示这个kernel的weight被置零了，控制变量比较消融后模型acc的变化，这样就能找到哪些kernel置零既能加快模型速度减少参数又能尽量维持acc。</p>
<table>
<thead>
<tr>
<th align="center">kernel1</th>
<th align="center">kernel2</th>
<th align="center">kernel3</th>
<th align="center">ACC</th>
</tr>
</thead>
<tbody><tr>
<td align="center">√</td>
<td align="center">√</td>
<td align="center">√</td>
<td align="center">90%</td>
</tr>
<tr>
<td align="center"></td>
<td align="center">√</td>
<td align="center">√</td>
<td align="center">88%</td>
</tr>
<tr>
<td align="center"></td>
<td align="center"></td>
<td align="center">√</td>
<td align="center">85%</td>
</tr>
</tbody></table>
<p>​    消融实验的维度：</p>
<pre><code>1. 每个weight中的值，一般是设置threshold来判断是否置零。
2. 每个kernel。
3. 每组kernel...
</code></pre>
<p><img src="C:\Users\user\AppData\Roaming\Typora\typora-user-images\image-20210224143108958.png" alt="image-20210224143108958"></p>
<p><img src="C:\Users\user\AppData\Roaming\Typora\typora-user-images\image-20210224143131588.png" alt="image-20210224143131588"></p>
<p><img src="C:\Users\user\AppData\Roaming\Typora\typora-user-images\image-20210224143140588.png" alt="image-20210224143140588"></p>
<h4 id="b-裁剪的方法—基于L1-norm的模型压缩"><a href="#b-裁剪的方法—基于L1-norm的模型压缩" class="headerlink" title="b. 裁剪的方法—基于L1 norm的模型压缩"></a>b. 裁剪的方法—基于L1 norm的模型压缩</h4><p>​    下图是基于L1 norm的模型压缩的原理思路。kernel matrix中，每一列代表一个filter，当某个filter被置零裁剪后，在卷积过后的下一层输出中，蓝色的那层feature map是不用计算的，于是会导致在再下一层的卷积运算中，所有filter的某层kernel，即下图第二个kernel matrix蓝色的那一行kernel，在卷积中是不用计算的，这使得最后输出的每张feature map中都有部分区域是不用计算的。这样就实现了只删除一次kernel matrix中的filter参数，却使得下两层卷积层的参数都减少了。</p>
<p><img src="C:\Users\user\AppData\Roaming\Typora\typora-user-images\image-20210224144958353.png" alt="image-20210224144958353"></p>
<p>​    最后，这篇文章的作者得出的结论是，裁剪m个filters，可以对第i层和第i+1层降低m/ni+1的计算量，n代表channel数。</p>
<p>​    如果要对上图ni+2层的kernel matrix，即第二个kernel matrix也要进行裁剪的话，就会如下图所示。绿色那列的filter被裁剪后，输出的feature map对应的某层也是不用计算的。得出的结论是会将ni+1 x ni+2维度的kernel matrix的计算量减小成（ni+1 -1） x （ni+2 -1）。</p>
<p><img src="C:\Users\user\AppData\Roaming\Typora\typora-user-images\image-20210224152438117.png" alt="image-20210224152438117"></p>
<p>​    对于ResNet这个有shortcut结构的网络，在对kernel matrix进行裁剪的时候，也要考虑到shortcut和裁剪对模型参数计算共同的影响。</p>
<p><img src="C:\Users\user\AppData\Roaming\Typora\typora-user-images\image-20210224152448268.png" alt="image-20210224152448268"></p>
<h3 id="c-另一种模型压缩方法，按weight值Prune"><a href="#c-另一种模型压缩方法，按weight值Prune" class="headerlink" title="c. 另一种模型压缩方法，按weight值Prune"></a>c. 另一种模型压缩方法，按weight值Prune</h3><p>​    基于L1 norm的模型压缩是在每组kernel层面上进行Prune，而有一种按weight值Prune的方法叫《Learning both Weights and Connections of Efficient Neural Network》，原理是给kernel中的weight设置一个阈值，将低于阈值的weight置零来达到Prune的效果。</p>
<h3 id="d-模型裁剪总结"><a href="#d-模型裁剪总结" class="headerlink" title="d. 模型裁剪总结"></a>d. 模型裁剪总结</h3><ol>
<li>计算各层内各组卷积核的权重和S。</li>
<li>把每层中的各个S在层内排序。</li>
<li>在每个层中，Prune(裁剪)掉m个S值最小的卷积组，同时下一层中对应的卷积核也Prune掉。</li>
</ol>
<p>注：单独的置零裁剪无法加速计算，因为0还是会参与运算，可调用一些系数系数计算库进行加速。</p>
<h2 id="Activation-Function"><a href="#Activation-Function" class="headerlink" title="Activation Function"></a>Activation Function</h2><h3 id="ReLU-Rethinking"><a href="#ReLU-Rethinking" class="headerlink" title="ReLU-Rethinking"></a>ReLU-Rethinking</h3><p>relu现在非常非常的常用，那它是不是就是最好的损失函数不需要再改良了呢？换句话说relu有没有什么缺点呢？</p>
<p>a. relu 在0处不可导；</p>
<p>b. 值只在0的一遍，在bp的时候会出现zigzag的情况，落在局部最优解风险更高；</p>
<p>c. 在小于零的部分为0，一定会有信息损耗；</p>
<h4 id="ReLU-Family"><a href="#ReLU-Family" class="headerlink" title="ReLU Family"></a>ReLU Family</h4><p>​    Leaky ReLU/PReLU，ELU,  CeLU,  SeLU…目标是不出现none-zero centered的情况。</p>
<h3 id="Swish-—-Self-gated-activation-Function-损失函数"><a href="#Swish-—-Self-gated-activation-Function-损失函数" class="headerlink" title="Swish — Self-gated activation Function 损失函数"></a>Swish — Self-gated activation Function 损失函数</h3><p>​    Swish的数学表示为 𝑓(𝑥)=𝑥∙𝜎(𝑥)，其实就是x乘上Sigmoid函数。Swish的Self-gated activation的想法是受到了LSTM的启发。</p>
<p>​    𝑓′𝑥=𝜎𝑥+𝑥∙𝜎𝑥∙1−𝜎𝑥=𝑥∙𝜎𝑥+𝜎𝑥1−𝑥∙𝜎𝑥=𝑓𝑥+𝜎𝑥∙(1−𝑓(𝑥))</p>
<p>​    Swish的性质：a. 在正半轴是无边界的（Unbounded above），可以避免梯度饱和；b. 在负半轴是有边界的（Bounded below），并且趋向于0，有很强的正则化效果；c. 非单调函数，强非线性，提高网络拟合复杂数据的能力；d. 非常的平滑，处处连续可到，容易走到全局最优解。</p>
<h3 id="Mish-可能是现在最好的损失函数"><a href="#Mish-可能是现在最好的损失函数" class="headerlink" title="Mish 可能是现在最好的损失函数"></a>Mish 可能是现在最好的损失函数</h3><p>Mish 的数学表达式为𝑓𝑥=𝑥∙𝑡𝑎𝑛ℎ(𝜍(𝑥))，其中 𝜍(𝑥)=𝑙𝑛(1+𝑒𝑥)—softplus。</p>
<p>![](F:\blog\CV\Algorithm Trick\pics\mish-derivative.png)</p>
<p>性质：a. 跟Swish有同样的效果，且在Swish效果不好的时候可能效果会更好；b. 比ReLU慢; c. 不能100%保证效果会好; d. 可以跟Ranger Optimizer一起运行;</p>
<h2 id="Loss-Function"><a href="#Loss-Function" class="headerlink" title="Loss Function"></a>Loss Function</h2><h3 id="L1-Loss-MAE-Mean-Absolute-Error"><a href="#L1-Loss-MAE-Mean-Absolute-Error" class="headerlink" title="L1 Loss (MAE: Mean Absolute Error)"></a>L1 Loss (MAE: Mean Absolute Error)</h3><p>​    L1 Loss的主要问题为： 导数为常数，这会导致模型很难在高准确率的同时收敛，容易在最优解附近震荡。</p>
<h3 id="L2-Loss-MSE-Mean-Squared-Error"><a href="#L2-Loss-MSE-Mean-Squared-Error" class="headerlink" title="L2 Loss (MSE: Mean Squared Error)"></a>L2 Loss (MSE: Mean Squared Error)</h3><p>​    L2 Loss的主要问题为：在训练开始的时候，导数会非常大，导致训练不稳定，容易迈出山谷，而在训练收敛得时候训练速度越来越慢。</p>
<h3 id="Smooth-L1-Loss"><a href="#Smooth-L1-Loss" class="headerlink" title="Smooth L1 Loss"></a>Smooth L1 Loss</h3><p>​    将L1 Loss和L2 Loss结合，Smooth L1 Loss解决的L1 Loss 和 L2 Loss的问题。它很像1964年提出的HUber Loss。</p>
<img src="F:\blog\CV\Algorithm Trick\pics\smooth-l1.png" style="zoom: 33%;" />

<h3 id="Summary"><a href="#Summary" class="headerlink" title="Summary"></a>Summary</h3><p>​    L1 Loss、L2 Loss 、Smooth L1 Loss在目标检测中的问题：1. 它们在预测中，四元坐标是分别单独计算的；2. 由于坐标单独预测，所以它们预测出不同的bbox时有可能会产生相同的loss。</p>
<h3 id="IoU-Loss"><a href="#IoU-Loss" class="headerlink" title="IoU Loss"></a>IoU Loss</h3><p>​    IoU Loss可以将四元坐标做统一预测，常用的IoU Loss一般已经不用ln函数了，直接为 - IoU 或者 1 - IoU。</p>
<p>![](F:\blog\CV\Algorithm Trick\pics\iou-loss.png)</p>
<p>​    IoU Loss 有两大问题。第一，如果预测出的bbox与ground truth不相交怎么办？这样不论两个框离得多远Loss都是1。如下图，明显左边的predicted bbox要比右边的好，但是它们Loss都是相等的，这很不合理。</p>
<img src="F:\blog\CV\Algorithm Trick\pics\iou-problem.png" style="zoom:33%;" />

<p>​    第二，如果bbox与ground truth相交，但是IoU值相等，也不能提供那个bbox更好地有效信息。如下图。</p>
<img src="F:\blog\CV\Algorithm Trick\pics\iou-problem2.png" style="zoom:33%;" />

<h3 id="GIoU-Loss"><a href="#GIoU-Loss" class="headerlink" title="GIoU Loss"></a>GIoU Loss</h3><p>​    GIoU Loss 解决了 IoU Loss 不相交计算问题，它的计算公式如下：</p>
<img src="F:\blog\CV\Algorithm Trick\pics\giou-formula.png" style="zoom:33%;" />

<p>​    用下图举例来解释这些公式。首先找到一个包含ground truth和predicted bbox的最小box，也就是找到这两个box中的最大和最小的坐标，如下图的蓝框。然后计算蓝色部分面积Ac。公式中的U等于两个框的面积和减去相交的面积，如下图二中黄的做斜杠的面积。Ac - U为蓝色矩形框中没有被两个框覆盖到的面积。GIoU就是IoU减去蓝色框中没有被两个框覆盖到的面积与蓝色框面积之比。GIoU Loss = 1 - GIoU。</p>
<img src="F:\blog\CV\Algorithm Trick\pics\Giou-loss.png" style="zoom:33%;" />

<p>​    GIoU Loss的问题是，有可能出现相交的面积相等但在不同位置的情况，如下图，三个框的位置计算的GIoU Loss 值是一样的。</p>
<img src="F:\blog\CV\Algorithm Trick\pics\giou-problem.png" style="zoom:33%;" />

<pre><code>### DIoU Loss
</code></pre>
<p>​    DIoU Loss添加了两个box的位置关系，可以解决这个问题。用两个box的中心距离d的平方除以两个box的对角线距离c的平方。这个值就是图中公式Rdiou，这是一个惩罚项，取值从0到趋向于1。DIoU Loss = 1 - IoU + Rdiou。</p>
<p>![](F:\blog\CV\Algorithm Trick\pics\diou-loss.png)</p>
<p>​    但是如果中点重合，但是Box大小相等但形状不一样，如下图，那DIoU就又无法表示这两种情况Loss的差别了。DIoU的作者提出了这样一个准则，Loss要考虑到相交面积，中心点距离和生成box的形状。DIoU的问题是没考虑到生成bbox的形状所导致的。</p>
<h3 id="CIoU-Loss"><a href="#CIoU-Loss" class="headerlink" title="CIoU Loss"></a>CIoU Loss</h3><p>​    CIoU Loss是IoU Loss家族中比较完善地解决各种情况的损失函数。CIoU的惩罚项Rciou = Rdiou + av。如下图公式所示，v的值是ground truth和predicted bbox的arctan角度差的平方乘上4/pi的平方，可以认为这是个非严格normalize的常数，因为arctan取值为（-pi/2, pi/2)，平方后的倒数就是4/pi的平方。</p>
<p>​    ![](F:\blog\CV\Algorithm Trick\pics\ciou-formula.png)</p>
<p>​    a是v的权重，它可以调节距离和比例的重要程度。如上公式，当bbox–&gt;ground truth的时候， 1- IoU就会趋向于0，a和v就趋近于1，Loss中bbox形状比例就更重要；反之，Loss中bbox的距离更重要。</p>
<p>​    CIoU Loss = DIoU Loss + av。它对模型的提升效果如下表。</p>
<p>![](F:\blog\CV\Algorithm Trick\pics\ciou-imporve.png)</p>
<h3 id="DIoU-NMS"><a href="#DIoU-NMS" class="headerlink" title="DIoU-NMS"></a>DIoU-NMS</h3><p>​    NMS的计算用到了IoU，DIoU-NMS就是在NMS算IoU的时候算一下DIoU的惩罚项Rdiou并减去，它的作用是当predicted Bbox离ground truth挺远的时候，保留它，近的就丢弃。</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://example.com/2021/04/29/CV/Algorithm-Trick/CV%20Algorithm%20Trick%20II/" data-id="cko2b5em100012wvcerg22qhg" data-title="" class="article-share-link">Teilen</a>
      
      
      
    </footer>
  </div>
  
    
<nav id="article-nav">
  
    <a href="/2021/04/29/CV/Autodriving/DeepLab/" id="article-nav-newer" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Neuer</strong>
      <div class="article-nav-title">
        
          (no title)
        
      </div>
    </a>
  
  
    <a href="/2021/04/29/hello-world/" id="article-nav-older" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Älter</strong>
      <div class="article-nav-title">Hello World</div>
    </a>
  
</nav>

  
</article>


</section>
        
          <aside id="sidebar">
  
    

  
    

  
    
  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archiv</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2021/04/">April 2021</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">letzter Beitrag</h3>
    <div class="widget">
      <ul>
        
          <li>
            <a href="/2021/04/29/CV/YOLO/Yolo%E7%B3%BB%E5%88%97%E6%A3%80%E6%B5%8B%E7%AE%97%E6%B3%95%E5%8E%9F%E7%90%86/">(no title)</a>
          </li>
        
          <li>
            <a href="/2021/04/29/CV/train/model-train/">(no title)</a>
          </li>
        
          <li>
            <a href="/2021/04/29/CV/train/data-preprocessing/">(no title)</a>
          </li>
        
          <li>
            <a href="/2021/04/29/CV/RCNN-family/RCNN%EF%BC%8CFast%20RCNN%EF%BC%8CFaster%20RCNN%E7%AE%97%E6%B3%95%E5%8E%9F%E7%90%86/">(no title)</a>
          </li>
        
          <li>
            <a href="/2021/04/29/CV/facebagnet&attention/SENet/">(no title)</a>
          </li>
        
      </ul>
    </div>
  </div>

  
</aside>
        
      </div>
      <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      
      &copy; 2021 Cheng Zixu<br>
      Powered by <a href="https://hexo.io/" target="_blank">Hexo</a>
    </div>
  </div>
</footer>

    </div>
    <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>
    


<script src="/js/jquery-3.4.1.min.js"></script>



  
<script src="/fancybox/jquery.fancybox.min.js"></script>




<script src="/js/script.js"></script>





  </div>
</body>
</html>