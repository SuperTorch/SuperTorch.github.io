<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  
  
  <title>Hexo</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
  <meta property="og:type" content="website">
<meta property="og:title" content="Hexo">
<meta property="og:url" content="http://example.com/index.html">
<meta property="og:site_name" content="Hexo">
<meta property="og:locale" content="en_US">
<meta property="article:author" content="Cheng Zixu">
<meta name="twitter:card" content="summary">
  
    <link rel="alternate" href="/atom.xml" title="Hexo" type="application/atom+xml">
  
  
    <link rel="shortcut icon" href="/favicon.png">
  
  
    
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/typeface-source-code-pro@0.0.71/index.min.css">

  
  
<link rel="stylesheet" href="/css/style.css">

  
    
<link rel="stylesheet" href="/fancybox/jquery.fancybox.min.css">

  
<meta name="generator" content="Hexo 5.4.0"></head>

<body>
  <div id="container">
    <div id="wrap">
      <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <div id="header-title" class="inner">
      <h1 id="logo-wrap">
        <a href="/" id="logo">Hexo</a>
      </h1>
      
    </div>
    <div id="header-inner" class="inner">
      <nav id="main-nav">
        <a id="main-nav-toggle" class="nav-icon"></a>
        
          <a class="main-nav-link" href="/">Home</a>
        
          <a class="main-nav-link" href="/archives">Archives</a>
        
      </nav>
      <nav id="sub-nav">
        
          <a id="nav-rss-link" class="nav-icon" href="/atom.xml" title="RSS Feed"></a>
        
        <a id="nav-search-btn" class="nav-icon" title="Suche"></a>
      </nav>
      <div id="search-form-wrap">
        <form action="//google.com/search" method="get" accept-charset="UTF-8" class="search-form"><input type="search" name="q" class="search-form-input" placeholder="Suche"><button type="submit" class="search-form-submit">&#xF002;</button><input type="hidden" name="sitesearch" value="http://example.com"></form>
      </div>
    </div>
  </div>
</header>

      <div class="outer">
        <section id="main">
  
    <article id="post-CV/YOLO/Yolo系列检测算法原理" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2021/04/29/CV/YOLO/Yolo%E7%B3%BB%E5%88%97%E6%A3%80%E6%B5%8B%E7%AE%97%E6%B3%95%E5%8E%9F%E7%90%86/" class="article-date">
  <time class="dt-published" datetime="2021-04-29T03:06:25.608Z" itemprop="datePublished">2021-04-29</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <p>[TOC]</p>
<h1 id="Yolo系列检测算法原理"><a href="#Yolo系列检测算法原理" class="headerlink" title="Yolo系列检测算法原理"></a>Yolo系列检测算法原理</h1><p>​    当two-stage（proposal + detection）系列的算法在目标检测界大展身手的时候，不禁会有人问道：”Why we have to train proposal first ? “。于是，便有学者展开了关于one-stage算法的研究，希望能研究出比two-stage运算效率更快的one-stage算法。</p>
<p>​    于是 Yolo 就这么诞生了，作者Joseph还挺有个性的，起名Yolo意思是” You Only Look Once ! “，它可以一次新预测多个Bbox的位置和类别，并且实现了end-to-end的检测和识别。它的运算速度Yolo算法经过一系列的发展，已经发展到了 V5 版本，也推动了One Stage Detection算法的发展。</p>
<h2 id="一-YOLO-V1-2015-Joseph"><a href="#一-YOLO-V1-2015-Joseph" class="headerlink" title="一. YOLO V1 [2015, Joseph]"></a>一. <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1506.02640.pdf">YOLO V1</a> [2015, Joseph]</h2><h3 id="1-YOLO-V1算法流程及原理"><a href="#1-YOLO-V1算法流程及原理" class="headerlink" title="1. YOLO V1算法流程及原理"></a>1. YOLO V1算法流程及原理</h3><p>​    YOLO V1算法流程如下图，主要步骤为：1）将输入图片划分成SxS的cell；2）通过CNN计算，预测出Bbox以及Confidence，同时对预测存在物体的Bbox计算后验概率；3）将前两步的计算结果结合起来便可以得到检测结果。</p>
<p><img src="F:\blog\CV\YOLO系列\pics\YOLOv1.png"></p>
<h4 id="1）将输入的图片划分成SxS的cell"><a href="#1）将输入的图片划分成SxS的cell" class="headerlink" title="1）将输入的图片划分成SxS的cell"></a>1）将输入的图片划分成SxS的cell</h4><p>​    首先将输入的图片划分成SxS的cell（在这里为7x7的cell），若某个物体的ground truth的中心点（x，y）落在这49个cell的某个内，则这个cell只负责预测这个物体，不会预测其他物体，且其他48个cell都不会预测这个物体。</p>
<p><img src="F:\blog\CV\YOLO系列\pics\YOLOv1-2.png"></p>
<p>​    如上图，红色的cell只负责预测汽车，不会负责预测小狗和自行车，而蓝色和黄色的cell也都不负责预测汽车。同理蓝色和黄色的cell只负责预测小狗和自行车。</p>
<h4 id="2）CNN预测Bbox"><a href="#2）CNN预测Bbox" class="headerlink" title="2）CNN预测Bbox"></a>2）CNN预测Bbox</h4><p>​    将输入图片划分成7x7的cell后，在图片经过CNN的运算后，每个cell都会预测B个Bbox和Bbox的confidence，即每个预测Bbox都有5个参数，即Bbox的四元坐标(x, y, w, h)，和该Bbox的confidence。在这里confidence。这里confidence的计算公式为( Pr(Object) * IOU(pred|truth) )，其中Pr(Object)指的是这个cell中是否有ground truth的中心点，存在则值为1，否则为0；IOU(pred|truth)便是预测的Bbox和ground truth计算的IoU值。若数据集中的检测物体有C个分类，于是每一个cell都会预测一次cell中物体属于某一类别的后验概率，即：Pr(Class_i|Object), i=1,2,…,C。而这个Cell中的B个预测的Bbox都会共享这个后验概率。</p>
<p>​    如果每个网格预测2个Bbox (B=2)，有20类待检测的目标（C=20），则相当于最终CNN预测的输出为一个长度为SxSx(Bxx5+C)=7x7x30的向量，从而完成检测+识别任务。</p>
<p>​    YOLO V1的网络结构如下，借鉴了LeNet的思想，但又有些细节改变。使用了24个卷积层，2个全连接层，不同的是用1×1的reduction layers紧跟3×3的convolutional layers取代了Lenet的inception modules 。</p>
<p><img src="F:\blog\CV\YOLO系列\pics\YOLOv1-cnn.png"></p>
<p>​    在检测的时候，每个cell的后验概率和confidence相乘，即Pr(Class_i|Object) * [Pr(Object) * IOU(pred|truth)] = Pr(Class_i) * IOU(pred|truth)，得到的结果是每个Bbox对每个类别的confidence score。最后通过阈值，去掉confidence score比较低的检测框，再通过NMS去掉冗余的检测框，就完成了检测。</p>
<h3 id="2-YOLO-V1的损失函数"><a href="#2-YOLO-V1的损失函数" class="headerlink" title="2. YOLO V1的损失函数"></a>2. YOLO V1的损失函数</h3><p>​    YOLO V1的损失函数分为3个部分：预测Bbox坐标的损失函数、预测Bbox置信度的损失函数和分类的损失函数。</p>
<p><img src="F:\blog\CV\YOLO系列\pics\loss1.png"></p>
<p><img src="F:\blog\CV\YOLO系列\pics\loss2.png"></p>
<p><img src="F:\blog\CV\YOLO系列\pics\loss3.png"></p>
<h3 id="3-YOLO-V1的问题"><a href="#3-YOLO-V1的问题" class="headerlink" title="3. YOLO V1的问题"></a>3. YOLO V1的问题</h3><p>​    毕竟是YOLO的第一个版本，虽然检测速度有了很大的提升，但存在着一些待解决的问题。</p>
<p>1）对靠近的物体检测不理想</p>
<p>​    毕竟只有7x7共49个cell，每个cell只预测一个物体，因此对于拥挤靠近的物体，它们的ground truth中心可能在同一个cell中，导致了预测效果不太理想。</p>
<p>2）对小物体检测不理想</p>
<p>​    好像是因为损失函数设计的原因</p>
<p>3）对同一类别不同长宽比的物体检测不理想</p>
<p>​    没用到anchor的锅…</p>
<p>4）没用到BN层</p>
<h2 id="二-YOLO-V2-2016-Joseph"><a href="#二-YOLO-V2-2016-Joseph" class="headerlink" title="二. YOLO V2 [2016, Joseph]"></a>二. <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1612.08242.pdf">YOLO V2</a> [2016, Joseph]</h2><p>​    YOLO V2针对上一版本留下的问题，一一作出了解决：加了BN层、加了anchor、改成FCN结构可以多尺度训练等…</p>
<h3 id="1-YOLO-V2的改进策略"><a href="#1-YOLO-V2的改进策略" class="headerlink" title="1. YOLO V2的改进策略"></a>1. YOLO V2的改进策略</h3><p>​    从下图可以看到YOLO V2在前一个版本的基础上增加了不少优化技巧，每个技巧都能让识别精度相比于第一个版本有着不少的提高</p>
<p><img src="F:\blog\CV\YOLO系列\pics\YOLOv2.jpg"></p>
<h4 id="1）Batch-Normalization"><a href="#1）Batch-Normalization" class="headerlink" title="1）Batch Normalization"></a>1）Batch Normalization</h4><p>​    在CNN进行运算的时候，每次卷积层过后输入的数据分布会一直在改变，这会使得训练的难度增大。BN层的原理就是将卷积过后的batch的输入重新规范化到均值为0，方差为1的标准正太分布。它能加快网络的训练速度和模型的收敛，同时有助于模型的正则化，可以舍弃掉dropout的同时防止模型的过拟合。YOLO V2的网络结构在每个卷积层后面都加了Batch Normalization。</p>
<h4 id="2）High-Resolution-Classifier"><a href="#2）High-Resolution-Classifier" class="headerlink" title="2）High Resolution Classifier"></a>2）High Resolution Classifier</h4><p>​    中文的意思叫高分辨率分类器，其实就是把ImageNet的图片数据分辨率提高后再训练一次。由于历史的原因，ImageNet的图片分辨率224x224太低了，因此可能会影响检测的精度。于是作者采用的训练方法是先在224x224的ImageNet上训练一次，然后把ImageNet的图片分辨率提升到448x448后进行Fine-tune训练，最后再用自己的数据集进行Fine-tune训练，最终得到的feature map尺寸为13x13。这一下就让模型在VOC2007上的mAP提升了3.7%。</p>
<h4 id="3）Convolutional-With-Anchor-Boxes"><a href="#3）Convolutional-With-Anchor-Boxes" class="headerlink" title="3）Convolutional With Anchor Boxes"></a>3）Convolutional With Anchor Boxes</h4><p>​    YOLO V1的网络结构最后用的是全连接层对Bbox进行预测，其中边界框的宽与高与输入图片的大小相关，而输入图片中可能存在不同尺度和长宽比（scales and ratios）的物体，折让YOLOv1在训练过程中学习不同物体的形状是比较困难的。于是YOLO V2借鉴了Faster-RCNN中RPN的anchor，移除了全连接层，采用了全卷积加设置不同比例和尺度的anchor box，再通过offset回归预测Bbox，使得模型更容易学习不同尺度和比例的物体。</p>
<h4 id="4）Dimension-Clusters"><a href="#4）Dimension-Clusters" class="headerlink" title="4）Dimension Clusters"></a>4）Dimension Clusters</h4><p>​    这个是YOLO V2生成anchor的一个小技巧，Faster-RCNN中的anchor是人为规定的9个。实际上如果anchor的尺度比较合适，模型将会更容易学习，从而做出更精确的预测。因此，YOLO V2的anchor不是人为设定的，而是算出来的。作者通过K-means聚类，对训练集中的数据的ground truth boxes根据公式：d * (box, centroid) = 1 − IOU(box, centroid），对BBox与中心Box的IoU值进行了聚类。最终，作者得出的结论是，我用5个anchor就行了，不需要到9个那么多。对于COCO数据集，每个anchor的width和height为：(0.57273, 0.677385), (1.87446, 2.06253), (3.33843, 5.47434), (7.88282, 3.52778), (9.77052, 9.16828)。这个维度是相对于卷积过后的特征图13x13的维度。</p>
<img src="F:\blog\CV\YOLO系列\pics\YOLOv2-anchor.png" style="zoom:50%;" />

<h4 id="5）Anchor，Truth-Bboxes-amp-Predicted-Bboxes"><a href="#5）Anchor，Truth-Bboxes-amp-Predicted-Bboxes" class="headerlink" title="5）Anchor，Truth Bboxes &amp; Predicted Bboxes"></a>5）Anchor，Truth Bboxes &amp; Predicted Bboxes</h4><h4 id="6）passthrough-amp-Fine-Grained-Features"><a href="#6）passthrough-amp-Fine-Grained-Features" class="headerlink" title="6）passthrough &amp; Fine-Grained Features"></a>6）passthrough &amp; Fine-Grained Features</h4><p>​    中文叫更精细的特征图，用于对小物体的预测。若YOLOv2的输入图片大小为416x416，经过卷积运算之后得到大小为13x13的特征图，这对检测大物体是足够了，但是对于小物体还需要更精细的特征图。YOLO V2引入了类似于ResNet中shortcut的passthrough层，将卷积过程中更高维度的特征图（如26x26）与后面低维度的特征图进行concat。那不同维度的feature map是怎么concat的呢？这里作者采用了叫reorg的算法，将高维度的特征图按位置抽样，且不丢失位置信息，如下图。4x4的feature map可以变成4个2x2的feature map，叠起来后这样就能实现高维度的特征图和低维度的特征图进行concat，从而实现Fine-Grained Features。</p>
<p><img src="F:\blog\CV\YOLO系列\pics\reorg.png"></p>
<h4 id="7）Multi-Scale-Training"><a href="#7）Multi-Scale-Training" class="headerlink" title="7）Multi-Scale Training"></a>7）Multi-Scale Training</h4><p>​    因为移除了全连接层，YOLO V2现在是个全卷积网络的结构，于是网络的输入不再需要固定维度。为了增强模型的鲁棒性，YOLO V2网络训练的输入图片尺寸为从320，352，…，到608，都是32的倍数，这是为了保证卷积过后的特征图尺度为整数。输入图片最小为320x320，此时对应的特征图大小为10x10，而输入图片最大为608x608，对应的特征图大小为19x19。在训练过程，每隔10个epoch改变一次输入图片大小，然后只需要修改对最后检测层的处理就可以继续训练。</p>
<h4 id="8）DarkNet-19"><a href="#8）DarkNet-19" class="headerlink" title="8）DarkNet-19"></a>8）DarkNet-19</h4><h3 id="2-YOLO-9000简介"><a href="#2-YOLO-9000简介" class="headerlink" title="2. YOLO 9000简介"></a>2. YOLO 9000简介</h3><p>​    YOLO V2的论文就叫YOLO9000:Better, Faster, Stronger, 所以YOLO 9000是在YOLO V2的基础上提出的一种可以检测超过9000个类别的模型，其主要贡献点在于提出了一种分类和检测的联合训练策略。众多周知，检测数据集的标注要比分类数据集打标签繁琐的多，所以ImageNet分类数据集比VOC等检测数据集的数量和种类高出几个数量级。而在YOLO V2中，边界框的预测其实并不依赖于物体的标签，所以YOLO V2可以实现在分类和检测数据集上的联合训练。对于检测数据集，可以用来学习预测物体的边界框、置信度以及为物体分类，而对于分类数据集可以仅用来学习分类，但是其可以大大扩充模型所能检测的物体种类。</p>
<h2 id="三-YOLO-V3-2018-Joseph"><a href="#三-YOLO-V3-2018-Joseph" class="headerlink" title="三. YOLO V3 [2018, Joseph]"></a>三. <a target="_blank" rel="noopener" href="https://pjreddie.com/media/files/papers/YOLOv3.pdf">YOLO V3</a> [2018, Joseph]</h2><h3 id="1-YOLO-V3的改进策略"><a href="#1-YOLO-V3的改进策略" class="headerlink" title="1. YOLO V3的改进策略"></a>1. YOLO V3的改进策略</h3><p>​    相对于YOLO V2，YOLO V3又顺应着时代的洪流加入了许多改进的策略，最主要的改进有：1. 新的残差结构；2. 多尺度结构，运用了FPN那样的up-to-down和down-to-up数据通路；3.改变了分类器</p>
<h2 id="四-YOLO-V4-2020-Alexey"><a href="#四-YOLO-V4-2020-Alexey" class="headerlink" title="四. YOLO V4 [2020, Alexey]"></a>四. <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2004.10934.pdf">YOLO V4</a> [2020, Alexey]</h2><p>​    2020年初，YOLO的作者宣布退休了，但是YOLO的发展脚步还是没有停下。这不，YOLO V4 虽然作者变了，不过好像是有原作者的授权的，YOLO精神得以延续！</p>
<h3 id="1-YOLO-V4的改进策略"><a href="#1-YOLO-V4的改进策略" class="headerlink" title="1. YOLO V4的改进策略"></a>1. YOLO V4的改进策略</h3><h4 id="1-Data-augmentation"><a href="#1-Data-augmentation" class="headerlink" title="1) Data augmentation"></a>1) Data augmentation</h4><p>​    YOLO V4在训练数据层面提出了几种新的数据增强的方法，就是把Mixup和Cutout结合起来的CutMix。</p>
<h5 id="a-Mixup-2018-Zhang"><a href="#a-Mixup-2018-Zhang" class="headerlink" title="a. Mixup [2018, Zhang]"></a>a. <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1710.09412.pdf">Mixup</a> [2018, Zhang]</h5><p>​    首先来了解一下Mixup，顾名思义就是把两张图按一定的比例混在一起。Mixup的作者对各种Mixup方法做了一系列实验后，总结出来Mixup的一些策略。他表示，Mixup两张图片就行了，三张或者更多效果差不多反而还花费更多的时间；只Mix数据集里的一个batch就行了，没必要Mix真个数据集，效果差不多；最好Mix标签不同的图面，如果Mix标签相同的图片没有提升。不过Mixup的提出主要是用来做分类而不是检测的。</p>
<img src="F:\blog\CV\YOLO系列\pics\mixup.png" style="zoom: 33%;" />

<p>​    Mixup有两种方法，见code</p>
<h5 id="b-Cutout-2017-DeVires"><a href="#b-Cutout-2017-DeVires" class="headerlink" title="b. Cutout [2017, DeVires]"></a>b. <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1708.04552.pdf">Cutout</a> [2017, DeVires]</h5><p>​    Cutout，顾名思义就是裁剪攻击，将图片的某一块裁剪掉，增大了图片的学习难度，对部分遮挡的识别能有一定的效果提升。</p>
<img src="F:\blog\CV\YOLO系列\pics\cutout.png" style="zoom: 25%;" />

<p>​    Cutout要先规范化图片到同一尺度，才能进行裁剪，这样是为了避免潜在的影响。之后又有人觉得裁剪后的像素用0填充对图片影响太大了，图片突然出现一块黑可能会于是提议用灰色填充。之后，又有人提出了随机擦除的方法，如上图左下。后来，有人觉得随机擦除的攻击太简单了，很容易把图像的重要特征给擦除了，于是提出了Gridmask的cutout方法，在图片上有规律地每次取一小块，如上图右下，这样既能保证重要特征不会被彻底擦除，又能cutout掉足够多的内容。</p>
<h5 id="c-CutMix"><a href="#c-CutMix" class="headerlink" title="c. CutMix"></a>c. CutMix</h5><p>​    CutMix就是把一张图片cut掉的部分用另一张图片的部分来mix上去填充，这样既能保证图片所有信息都是有用的，并且有区域性dropout的功能。作者做了一系列的试验后表示，CutMix的引入能让模型效果明显好于Mixup；其次CutMix也是组合两张图片就足够了。</p>
<img src="F:\blog\CV\YOLO系列\pics\cutmix.png" style="zoom:33%;" />

<h4 id="2）Regularization"><a href="#2）Regularization" class="headerlink" title="2）Regularization"></a>2）Regularization</h4><p>​    YOLO V4在正则化方面做了两个主要优化措施，分别是Label smoothing和Dropblock。</p>
<h5 id="a-label-smoothing"><a href="#a-label-smoothing" class="headerlink" title="a. label smoothing"></a>a. label smoothing</h5><p>​    label smoothing的意思就是别把标签定得太死。举个例子，就是原本的二分类0/1标签，经过label smoothing把标签定得没这么死后，变成了0.05/0.95，或者0.1/0.9，而不是彻底的0和1。</p>
<img src="F:\blog\CV\YOLO系列\pics\label-smoothing.png" style="zoom:50%;" />

<p>​    Label Smoothing的效果如上图所示，将CIFAR和ImageNet的数据进行Label Smoothing后再进行训练，可以发现训练集和验证集的结果显示，类别之间的特征差异更加明显。</p>
<h5 id="b-Dropblock"><a href="#b-Dropblock" class="headerlink" title="b. Dropblock"></a>b. Dropblock</h5><p>​    Dropblock实际上就是把dropout的思想引入到卷积层而提出的。通常情况下，Dropout一般用于全连接层，且它有一定的正则作用可以防止模型过拟合。Dropout在卷积层是没有效果的，原因是因为卷积核共享参数，这样在卷积层中随意Dropout掉的神经元在其他神经元中会保留有信息。但是在YOLO中采用的是全卷积网络，没有全连接层。想到达到有Dropout的正则化效果，就要使用DropBlock。</p>
<p>​    Dropout是随机将某个神经元失活。为了解决卷积层中参数共享的问题，DropBlock的原理是将feature map中某一片block给随机失活。</p>
<img src="F:\blog\CV\YOLO系列\pics\dropblock.png" style="zoom:33%;" />

<h4 id="3）Activation-Function"><a href="#3）Activation-Function" class="headerlink" title="3）Activation Function"></a>3）Activation Function</h4><h5 id="Mish"><a href="#Mish" class="headerlink" title="Mish"></a>Mish</h5><h4 id="4）Loss-Function"><a href="#4）Loss-Function" class="headerlink" title="4）Loss Function"></a>4）Loss Function</h4><h5 id="CIoU-Loss"><a href="#CIoU-Loss" class="headerlink" title="CIoU Loss"></a>CIoU Loss</h5><p>​    介绍CIoU Loss可以先从IoU Loss说起。</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://example.com/2021/04/29/CV/YOLO/Yolo%E7%B3%BB%E5%88%97%E6%A3%80%E6%B5%8B%E7%AE%97%E6%B3%95%E5%8E%9F%E7%90%86/" data-id="cko2b5emg000i2wvcgrj7agga" data-title="" class="article-share-link">Teilen</a>
      
      
      
    </footer>
  </div>
  
</article>



  
    <article id="post-CV/train/model-train" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2021/04/29/CV/train/model-train/" class="article-date">
  <time class="dt-published" datetime="2021-04-29T03:06:25.151Z" itemprop="datePublished">2021-04-29</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <h1 id="模型训练"><a href="#模型训练" class="headerlink" title="模型训练"></a>模型训练</h1><h2 id="训练流程概述"><a href="#训练流程概述" class="headerlink" title="训练流程概述"></a>训练流程概述</h2><h2 id="参数初始化-Initializer"><a href="#参数初始化-Initializer" class="headerlink" title="参数初始化 Initializer"></a>参数初始化 Initializer</h2><h3 id="Constant"><a href="#Constant" class="headerlink" title="Constant"></a>Constant</h3><h3 id="Random"><a href="#Random" class="headerlink" title="Random"></a>Random</h3><h3 id="Xavier"><a href="#Xavier" class="headerlink" title="Xavier"></a>Xavier</h3><h3 id="Kaiming"><a href="#Kaiming" class="headerlink" title="Kaiming"></a>Kaiming</h3><h3 id="Pre-trained-Weight"><a href="#Pre-trained-Weight" class="headerlink" title="Pre-trained Weight"></a>Pre-trained Weight</h3><h4 id="完全预训练"><a href="#完全预训练" class="headerlink" title="完全预训练"></a>完全预训练</h4><h4 id="部分与训练"><a href="#部分与训练" class="headerlink" title="部分与训练"></a>部分与训练</h4><h2 id="优化器-Optimizer"><a href="#优化器-Optimizer" class="headerlink" title="优化器 Optimizer"></a>优化器 Optimizer</h2><h4 id="SGD"><a href="#SGD" class="headerlink" title="SGD"></a>SGD</h4><h4 id="SGDM"><a href="#SGDM" class="headerlink" title="SGDM"></a>SGDM</h4><p>Stochastic Gradient Descent with Momentum</p>
<h4 id="NAG"><a href="#NAG" class="headerlink" title="NAG"></a>NAG</h4><p>Nesterov Accelerated Gradient 平均梯度</p>
<h4 id="AdaGrad"><a href="#AdaGrad" class="headerlink" title="AdaGrad"></a>AdaGrad</h4><p>Adaptive Gradient</p>
<p>缺点：随着t推移，a趋向于0，梯度消失，难以训练</p>
<h4 id="RMSProp"><a href="#RMSProp" class="headerlink" title="RMSProp"></a>RMSProp</h4><p>RootMeanSquare Propogation</p>
<h4 id="AdaDelta"><a href="#AdaDelta" class="headerlink" title="AdaDelta"></a>AdaDelta</h4><p>Adaptive Delta</p>
<h4 id="Adam"><a href="#Adam" class="headerlink" title="Adam"></a>Adam</h4><p>Adaptive Moments Estimation</p>
<p>比SGD快很多但收敛性没那么好</p>
<h4 id="L2正则化-Weight-Decay-AdamW"><a href="#L2正则化-Weight-Decay-AdamW" class="headerlink" title="L2正则化/ Weight Decay/AdamW"></a>L2正则化/ Weight Decay/AdamW</h4><p>L2正则化与Weight Decay是等价的</p>
<p>AdamW Adam with decoupled weight decay</p>
<h2 id="学习率调整-LR-Scheduler"><a href="#学习率调整-LR-Scheduler" class="headerlink" title="学习率调整 LR Scheduler"></a>学习率调整 LR Scheduler</h2><h3 id="衰减式"><a href="#衰减式" class="headerlink" title="衰减式"></a>衰减式</h3><h4 id="StepLR"><a href="#StepLR" class="headerlink" title="StepLR"></a>StepLR</h4><h4 id="MultiStepLR"><a href="#MultiStepLR" class="headerlink" title="MultiStepLR"></a>MultiStepLR</h4><h4 id="ExponentialLR"><a href="#ExponentialLR" class="headerlink" title="ExponentialLR"></a>ExponentialLR</h4><h3 id="循环式"><a href="#循环式" class="headerlink" title="循环式"></a>循环式</h3><h4 id="CosineAnnealingLR"><a href="#CosineAnnealingLR" class="headerlink" title="CosineAnnealingLR"></a>CosineAnnealingLR</h4><h4 id="CyclicLR"><a href="#CyclicLR" class="headerlink" title="CyclicLR"></a>CyclicLR</h4><h3 id="自适应"><a href="#自适应" class="headerlink" title="自适应"></a>自适应</h3><h4 id="ReduceLROnPlateau"><a href="#ReduceLROnPlateau" class="headerlink" title="ReduceLROnPlateau"></a>ReduceLROnPlateau</h4><p>•torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode=’min’, factor=0.1, patience=10, threshold=0.0001, threshold_mode=’rel’, cooldown=0, min_lr=0, eps=1e-08, verbose=False)</p>
<p>•Example: scheduler.step(val_loss)</p>
<h3 id="手动调整"><a href="#手动调整" class="headerlink" title="手动调整"></a>手动调整</h3><h2 id="损失函数"><a href="#损失函数" class="headerlink" title="损失函数"></a>损失函数</h2><h3 id="Cross-Entropy-Loss"><a href="#Cross-Entropy-Loss" class="headerlink" title="Cross Entropy Loss"></a>Cross Entropy Loss</h3><img src="F:\blog\CV\train\imgs\crossentropy.png" style="zoom:48%;" />

<p>Cross Entropy + Softmax =&gt; 得到一组和为1的概率数组。</p>
<h3 id="Binary-Cross-Entropy-Loss"><a href="#Binary-Cross-Entropy-Loss" class="headerlink" title="Binary Cross Entropy Loss"></a>Binary Cross Entropy Loss</h3><img src="F:\blog\CV\train\imgs\BCE.png" style="zoom:48%;" />

<p>BCE + Sigmoid =&gt; 得到一个(0, 1)之间的概率。</p>
<h3 id="Balance-Binary-Cross-Entropy-Loss"><a href="#Balance-Binary-Cross-Entropy-Loss" class="headerlink" title="Balance Binary Cross Entropy Loss"></a>Balance Binary Cross Entropy Loss</h3><img src="F:\blog\CV\train\imgs\BBCE.png" style="zoom:48%;" />

<p>一定程度上解决正负样本不平衡问题</p>
<h3 id="Focal-Loss"><a href="#Focal-Loss" class="headerlink" title="Focal Loss"></a>Focal Loss</h3><p><img src="F:\blog\CV\train\imgs\focalloss.png" alt="L=-α〖(1-p)〗^γ ylogp-(1-α) p^γ (1-y)log⁡(1-p)"></p>
<p>给难易样本加上权重γ，更关注难样本的学习。</p>
<p>同时考虑样本平衡与样本难易；样本平衡与样本难易有一定的关联。</p>
<h3 id="Dice-Loss"><a href="#Dice-Loss" class="headerlink" title="Dice Loss"></a>Dice Loss</h3><p>语义分割常用Loss</p>
<p><img src="F:\blog\CV\train\imgs\diceloss.png"></p>
<p>Dice Coefficient是IoU的分子分母都加上|X|∩|Y|，与IoU是正相关，Dice Loss是1 - Dice Coefficient，这么做是为了方便求导。</p>
<p>Dice Loss为了防止分母为0，通常在分母加上eps，或者加上拉普拉斯平滑即分子分母同时加上1。</p>
<p>在语义分割中，计算IoU的方法是|X|∩|Y| = ∑ gt * pred ，gt * pred得到某个像素的概率交集，每个像素求和得到整体的概率交集 ；|X|+|Y| = ∑ gt + pred。</p>
<h2 id="激活函数"><a href="#激活函数" class="headerlink" title="激活函数"></a>激活函数</h2><h3 id="Softmax"><a href="#Softmax" class="headerlink" title="Softmax"></a>Softmax</h3><img src="F:\blog\CV\train\imgs\softmax.png" style="zoom:48%;" />

<p>对其求导得：</p>
<img src="F:\blog\CV\train\imgs\softmax2.png" style="zoom:48%;" />

<h3 id="Sigmoid"><a href="#Sigmoid" class="headerlink" title="Sigmoid"></a>Sigmoid</h3><img src="F:\blog\CV\train\imgs\sigmoid.png" style="zoom:48%;" />

<img src="F:\blog\CV\train\imgs\sigmoid2.png" style="zoom:48%;" />



<h3 id="CrossEntropy-Softmax"><a href="#CrossEntropy-Softmax" class="headerlink" title="CrossEntropy + Softmax"></a>CrossEntropy + Softmax</h3><p>求导后化简的形式很漂亮。</p>
<img src="F:\blog\CV\train\imgs\ce+softmax.png" style="zoom:48%;" />

<h3 id="MSE-Softmax"><a href="#MSE-Softmax" class="headerlink" title="MSE + Softmax"></a>MSE + Softmax</h3><p>若使用MSE损失函数做分类，则求导有：</p>
<img src="F:\blog\CV\train\imgs\mse.png" style="zoom:20%;" />

<p>导数是3个(0, 1)之间的数相乘，容易造成梯度消失，因此分类不用MSE。</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://example.com/2021/04/29/CV/train/model-train/" data-id="cko2b5emg000h2wvc2cgm7bex" data-title="" class="article-share-link">Teilen</a>
      
      
      
    </footer>
  </div>
  
</article>



  
    <article id="post-CV/train/data-preprocessing" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2021/04/29/CV/train/data-preprocessing/" class="article-date">
  <time class="dt-published" datetime="2021-04-29T03:06:25.092Z" itemprop="datePublished">2021-04-29</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <h1 id="数据处理"><a href="#数据处理" class="headerlink" title="数据处理"></a>数据处理</h1><h2 id="数据集制作"><a href="#数据集制作" class="headerlink" title="数据集制作"></a>数据集制作</h2><h3 id="数据采集"><a href="#数据采集" class="headerlink" title="数据采集"></a>数据采集</h3><p>• 公开/竞赛数据集（注意商用协议/开源协议，使用不规范不能够上线）</p>
<p>• 收费/外包采集 （成本、资金、人力花费较高，也有内部采集方法）</p>
<p>• 网络爬虫（数据一般不干净）</p>
<p>• 用户数据（隐私协议，可以搜集用户发表的图片用于训练）</p>
<p>• 数据生成（成本最低，难度最高）</p>
<h3 id="数据标注"><a href="#数据标注" class="headerlink" title="数据标注"></a>数据标注</h3><h4 id="手工标注"><a href="#手工标注" class="headerlink" title="手工标注"></a>手工标注</h4><p>• 标注任务</p>
<p>​    • 分类-类标签</p>
<p>​    • 检测-矩形框</p>
<p>​    • 分割-多边形框</p>
<p>​        •cv2.fillPoly</p>
<p>• 标注工具</p>
<p>​    • LabelMe</p>
<p>​    • LabelImg</p>
<h4 id="自动标注"><a href="#自动标注" class="headerlink" title="自动标注"></a>自动标注</h4><p>• 定制化采集设备</p>
<p>​    • 3D采集、激光雷达</p>
<p>​    • 动捕系统</p>
<p>•数据生成</p>
<p>​    • 3D渲染</p>
<p>​    • 图片融合</p>
<p>​    • GAN</p>
<p>•模型预测</p>
<p>​    • 自有模型</p>
<p>​    • 开源模型</p>
<p>​    • 多模型预测</p>
<h3 id="数据集制作-1"><a href="#数据集制作-1" class="headerlink" title="数据集制作"></a>数据集制作</h3><p>• 样本唯一标识符（ID）</p>
<p>​    • 样本文件路径</p>
<p>​    • 样本采集时间戳</p>
<p>​    • 唯一数字（字母）ID</p>
<p>​    • Uniform Resource Locator (URL)</p>
<p>• 样本列表（.csv / .txt）</p>
<p>​    • 样本标注</p>
<p>​    • 数据划分：训练集/验证集/测试集</p>
<p>• 存储方式</p>
<p>​    • 文件存储</p>
<p>​    • Lightning Memory-Mapped Database (LMDB) 读取快，K-V形式。</p>
<p>​    • Hierarchical Data Format Version 5 (HDF5)</p>
<p>​    • TFRecord</p>
<p>​    • ……</p>
<h2 id="数据处理-1"><a href="#数据处理-1" class="headerlink" title="数据处理"></a>数据处理</h2><h3 id="Step-1-数据清洗"><a href="#Step-1-数据清洗" class="headerlink" title="Step 1 数据清洗"></a>Step 1 数据清洗</h3><p>• 清洗对象</p>
<p>​    • 错误标签</p>
<p>​    • 模糊样本</p>
<p>​    • 异常样本</p>
<p>• 清洗方式</p>
<p>​    • 人工筛查</p>
<p>​    • 自动筛查</p>
<p>​        • 离群点检测</p>
<p>​        • 难样本挖掘</p>
<p>​    • 自动+人工筛查</p>
<h3 id="Step-2-数据预处理"><a href="#Step-2-数据预处理" class="headerlink" title="Step 2 数据预处理"></a>Step 2 数据预处理</h3><p>预处理可以在训练/预测阶段进行。</p>
<p>• 裁剪/crop（图片过大、统一尺度、无用信息）</p>
<p>• 缩放/resize（图片过大、统一尺度（2n）、多尺度）</p>
<p>• 填充/padding（统一尺度、保持尺寸）</p>
<p>• 归一化/normalize（统一量级、分布假设）</p>
<p>• 直方图均衡化/equalization （对比度增强）多用于预测</p>
<p>• 白平衡/balance（色偏）多用于预测</p>
<p>• ……（具体问题/具体场景 – 具体分析）</p>
<h3 id="Step-3-数据扩增"><a href="#Step-3-数据扩增" class="headerlink" title="Step 3 数据扩增"></a>Step 3 数据扩增</h3><h4 id="几何变换"><a href="#几何变换" class="headerlink" title="几何变换"></a>几何变换</h4><p>• 裁剪</p>
<p>• 缩放</p>
<p>• 平移</p>
<p>• 旋转</p>
<p>• 翻转</p>
<p>• 剪切</p>
<p>• 透视</p>
<h4 id="颜色空间"><a href="#颜色空间" class="headerlink" title="颜色空间"></a>颜色空间</h4><p>• 对比度</p>
<p>• 饱和度</p>
<p>• 亮度/明度</p>
<p>• 色调</p>
<p>• 通道交换</p>
<p>• 直方图均衡化</p>
<p>• 颜色空间转化</p>
<p>​    • 灰度/HSV/Lab/YUV （YOLOv4）</p>
<h4 id="像素操作"><a href="#像素操作" class="headerlink" title="像素操作"></a>像素操作</h4><p>• 模糊</p>
<p>• 锐化</p>
<p>• 噪声</p>
<p>• 高斯/泊松/椒盐</p>
<p>• 随机擦除</p>
<p>​    • CutOut/DropBlock</p>
<h4 id="多图扩增"><a href="#多图扩增" class="headerlink" title="多图扩增"></a>多图扩增</h4><p>• MixUp</p>
<p>• CutMix</p>
<p>• Mosaic</p>
<h4 id="数据扩增开源库"><a href="#数据扩增开源库" class="headerlink" title="数据扩增开源库"></a>数据扩增开源库</h4><p>• Pillow</p>
<p>• Imgaug</p>
<p>• torchvision.transforms</p>
<p>• AutoAugment</p>
<h2 id="数据加载"><a href="#数据加载" class="headerlink" title="数据加载"></a>数据加载</h2><h1 id="数据闭环和主动学习"><a href="#数据闭环和主动学习" class="headerlink" title="数据闭环和主动学习"></a>数据闭环和主动学习</h1><h2 id="数据闭环"><a href="#数据闭环" class="headerlink" title="数据闭环"></a>数据闭环</h2><p><img src="F:\blog\CV\data-preprocessing\imgs\数据闭环.png"></p>
<h2 id="主动学习"><a href="#主动学习" class="headerlink" title="主动学习"></a>主动学习</h2><p><img src="F:\blog\CV\data-preprocessing\imgs\主动学习.png"></p>
<p>自动选择样最值得标注的样本是主动学习的关键。</p>
<p>1、增加数据的边际效益递减</p>
<p>2、数据量的需求存在临界值</p>
<p>3、用更低的成本达到数据临界值</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://example.com/2021/04/29/CV/train/data-preprocessing/" data-id="cko2b5emf000g2wvc1xhf90rz" data-title="" class="article-share-link">Teilen</a>
      
      
      
    </footer>
  </div>
  
</article>



  
    <article id="post-CV/RCNN-family/RCNN，Fast RCNN，Faster RCNN算法原理" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2021/04/29/CV/RCNN-family/RCNN%EF%BC%8CFast%20RCNN%EF%BC%8CFaster%20RCNN%E7%AE%97%E6%B3%95%E5%8E%9F%E7%90%86/" class="article-date">
  <time class="dt-published" datetime="2021-04-29T03:06:23.386Z" itemprop="datePublished">2021-04-29</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <p>[TOC]</p>
<h1 id="RCNN，Fast-RCNN，Faster-RCNN算法原理"><a href="#RCNN，Fast-RCNN，Faster-RCNN算法原理" class="headerlink" title="RCNN，Fast-RCNN，Faster-RCNN算法原理"></a>RCNN，Fast-RCNN，Faster-RCNN算法原理</h1><p>​    在传统的图像识别领域，图像检测一般有3个stage：1. 对输入图像使用传统数图像处理的方法，人为手动地提取图像特征；2.需要对提取的的特征进行仔细地筛选，选出有用的利于决策的特征；3.使用传统机器学习工具（如SVM），对这些特征进行分类和回归，进一步对目标进行识别。这个时期的图像识别往往需要大量的人力工程来提取和筛选图像特征，而且识别结果的精度也差强人意。        <img src="./pics/3-stage.png"></p>
<p>​    发展到深度学习时代后，图像检测的步骤开始简化成2个stage：1.使用CNN对输入的图像自动地提取特征，称为feature map；2.使用这些feature map构建分类器和回归器，进一步实现目标检测。这样的方法不仅能减少大量的人工特征提取工程量，识别结果的精度也有很大的提升。</p>
<p><img src=".%5Cpics%5C2-stage.png"></p>
<p>​    下文将讲解深度学习目标检测中，two-stage（Region Proposal + cls&amp;det）的经典算法：RCNN、Fast-RCNN、Faster-RCNN的算法原理，以及提出的新思想和解决的问题。</p>
<h2 id="一-RCNN-2014-Ross-—-深度学习图像检测开山之作"><a href="#一-RCNN-2014-Ross-—-深度学习图像检测开山之作" class="headerlink" title="一. RCNN [2014, Ross]—- 深度学习图像检测开山之作"></a>一. <a target="_blank" rel="noopener" href="https://arxiv.org/abs/1311.2524">RCNN</a> [2014, Ross]—- 深度学习图像检测开山之作</h2><h3 id="1-RCNN算法原理流程"><a href="#1-RCNN算法原理流程" class="headerlink" title="1. RCNN算法原理流程"></a>1. RCNN算法原理流程</h3><p>​    RCNN算法的流程大致分为3步：1. <strong>Region Proposal</strong>，对输入的图像提取区域候选框，每张图大概会生成2k个候选框；2. <strong>Feature Extraction</strong>，从候选框中截取图片，使用CNN计算出每个截取图片的feature map；3.<strong>Classification&amp;Detection</strong>，对feature map进行分类和检测。</p>
<p><img src=".%5Cpics%5CRCNN1.png"></p>
<h4 id="1）Region-Proposal"><a href="#1）Region-Proposal" class="headerlink" title="1）Region Proposal"></a>1）Region Proposal</h4><p>​    RCNN的region proposal采用的是selective search的方法，每张图片生成近2k的候选框。Selective search算法类似于一种无监督聚类，将图片相似且相邻的像素聚为一类，从而生成候选框。相似度的计算会考虑颜色、纹理等多个维度，目的是保持多样性的策略，尽可能地考虑图片的各种场景，以免漏掉要检测的目标。Selective search详细的算法原理可以参考这篇文章: <a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/27467369">Selective Search</a></p>
<p>![Selective Search](.\pics\Selective Search.png)</p>
<p>​    之后，通过生成的候选框，去原图截取图片块，对每个图片块进行resize成统一维度大小，用于下一步送入CNN中进行计算。</p>
<h4 id="2）Feature-Extraction"><a href="#2）Feature-Extraction" class="headerlink" title="2）Feature Extraction"></a>2）Feature Extraction</h4><p>​    用CNN提取特征部分，RCNN采用的是AlexNet网络结构。通过运算，最后每个候选区域会生成一个4096维的特征向量。</p>
<p>![](.\pics\Feature Extraction.png)</p>
<p>​    作者在这里运用了一些训练技巧，采用了AlexNet在ILSVRC2012的数据集ImageNet预训练好的模型参数，在真实的目标检测数据集里进行了Fine-tune。因为在ImageNet上训练的是模型识别物体种类的能力而不是预测检测框bbox位置的能力。其次，训练的每个batch大小为128，包括32个正样本和96个负样本，正负样本的生成是通过生成的候选区域与真实标签Ground Truth中的Box进行IoU运算，设置阈值threshold为0.5，IoU&gt;=0.5的便认定为正样本（物体），否则为负样本（背景）。通常正负样本的比例为1:3。</p>
<h4 id="3）Classification-amp-Detection"><a href="#3）Classification-amp-Detection" class="headerlink" title="3）Classification&amp;Detection"></a>3）Classification&amp;Detection</h4><p>​    在传统机器学习分类器仍作为主流的年代，作者采用的是使用SVM作为RCNN的分类器，然后通过NMS算法，筛出IoU最大的那个bbox，再进行bbox regression，得到更精确的目标检测框。</p>
<h5 id="a-SVM分类器"><a href="#a-SVM分类器" class="headerlink" title="a. SVM分类器"></a>a. SVM分类器</h5><p>​    首先为什么不直接采用AlexNet的Softmax进行分类，而另采用SVM作为分类器，作者给出的理由是AlexNet是专注于做分类的，对于bbox位置的预测不太准确。并且SVM采用的是hard negative mining的方法进行训练。hard negative就是那些容易让分类器分类错误的负样本，增大这类样本的数据量可以让模型更好地学习对目标的分类。</p>
<p><img src="F:\blog\CV\RCNN系列\pics\svm.png"></p>
<p>​    在训练SVM的时候，正样本采用的是Ground Truth。与CNN部分不同的是，负样本设置的阈值是IoU&lt;0.3，即与ground truth的IoU小于0.3的候选区域样本，其他的样本都忽略，这样有助于模型的收敛。在训练CNN的时候，我们需要的是更多的数据来学习提取图像的特征，但在SVM分类器中，过多的数据可能会损害模型精度，因此我们需要更难的数据，这样SVM的效果才会更好。</p>
<h5 id="b-NMS-非极大值抑制"><a href="#b-NMS-非极大值抑制" class="headerlink" title="b. NMS - 非极大值抑制"></a>b. NMS - 非极大值抑制</h5><p>​    NMS的目的就是去除冗余的候选框，保留最好的那个。NMS算法主要步骤为：1. 对每个进行打分，在RCNN中是根据分类预测的分数来进行打分的；2.找到分数最高的候选框，其余的候选框依次与该最高分候选框进行IoU值的计算；3. 如果某个候选框的IoU值大于设定好的阈值，则丢掉这个候选框；4.在剩下的候选框中（除去之前的分数最高候选框），找到剩下的分数最高的候选框，重复以上步骤，直到所有框都是被保留下来的最高分候选框。</p>
<p><img src="F:\blog\CV\RCNN系列\pics\NMS.png"></p>
<h5 id="c-Bbox-Regression"><a href="#c-Bbox-Regression" class="headerlink" title="c. Bbox Regression"></a>c. Bbox Regression</h5><p>​        先留个坑，后面再补</p>
<h3 id="2-RCNN留下的问题"><a href="#2-RCNN留下的问题" class="headerlink" title="2.RCNN留下的问题"></a>2.RCNN留下的问题</h3><h4 id="1）NMS的问题"><a href="#1）NMS的问题" class="headerlink" title="1）NMS的问题"></a>1）NMS的问题</h4><p>​    当两个物体非常接近，且区域有大部分重叠地时候，使用NMS算法筛选候选框可就出问题了。如下图所示：根据NMS算法的原理，如果蓝色候选框的分数更高，黄色的检测框将会被丢弃；若黄色候选框的分数更高，则蓝色的检测框将会被丢弃。因此要想个办法，把两个框都保留下来。</p>
<p><img src="F:\blog\CV\RCNN系列\pics\NMS-problem.png"></p>
<p>​        解决方法：</p>
<ul>
<li><a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1704.04503.pdf">Soft-NMS</a>[2017]</li>
<li><a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1807.11590.pdf">IoU-Net</a>[2018, Localization Confidence + PrROI Pooling]</li>
<li><a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1809.08545.pdf">Softer-NMS</a>[2019, KL-Loss + Softer-NMS]</li>
</ul>
<h4 id="2）运行速度问题"><a href="#2）运行速度问题" class="headerlink" title="2）运行速度问题"></a>2）运行速度问题</h4><p>​    RCNN算法需要把Region Proposal生成的2000个图片，都放到CNN去处理，才算训练完一张图片，因此运行的速度还比较慢，所以也叫Slow RCNN。</p>
<h4 id="3）SVM和Regressor的问题"><a href="#3）SVM和Regressor的问题" class="headerlink" title="3）SVM和Regressor的问题"></a>3）SVM和Regressor的问题</h4><p>​    SVM和Regressor是在CNN之后，两者是分开训练的，因此训练SVM和Regressor无法更新CNN的参数。作者后来认为这种方式挺离谱的，因此在之后的Fast-RCNN中改了过来。</p>
<h4 id="4）训练的pipline多级且复杂"><a href="#4）训练的pipline多级且复杂" class="headerlink" title="4）训练的pipline多级且复杂"></a>4）训练的pipline多级且复杂</h4><h2 id="二-Fast-RCNN-2015-Ross"><a href="#二-Fast-RCNN-2015-Ross" class="headerlink" title="二. Fast-RCNN [2015, Ross]"></a>二. <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1504.08083.pdf">Fast-RCNN</a> [2015, Ross]</h2><h3 id="1-Fast-RCNN算法原理流程"><a href="#1-Fast-RCNN算法原理流程" class="headerlink" title="1.Fast-RCNN算法原理流程"></a>1.Fast-RCNN算法原理流程</h3><p>​    对于RCNN巡行速度过慢等问题，作者提出了基于RCNN的改善模型Fast-RCNN。其中的主要改进有：1. 将classification和detection的部分融合到CNN中，不再使用额外的SVM和Regressor，极大地减少了计算量和训练速度；2. Selective Search后不再对region proposal得到的2k个候选框进行截取输入，改用ROI Project，将region proposal映射到feature map上；3. 使用ROI pooling 算法，支持输入不同尺度的图片。</p>
<p>​        具体的算法流程如下图，可分为B0-B5，共6个部分。</p>
<p><img src="F:\blog\CV\RCNN系列\pics\Fast-RCNN1.png"></p>
<h4 id="1）Region-Proposal：Same-as-RCNN"><a href="#1）Region-Proposal：Same-as-RCNN" class="headerlink" title="1）Region Proposal：Same as RCNN"></a>1）Region Proposal：Same as RCNN</h4><p>​    跟RCNN一样，Fast-RCNN采用的也是Selective Search的方法来产生Region Proposal，每张图片生成2k张图片。但是不同的是，之后不会对2k个候选区域去原图截取，后输入CNN，而是直接对原图进行一次CNN，在CNN后的feature map，通过ROI project在feature map上找到Region Proposal的位置。</p>
<h4 id="2）Convolution-amp-ROI-Project"><a href="#2）Convolution-amp-ROI-Project" class="headerlink" title="2）Convolution &amp; ROI Project"></a>2）Convolution &amp; ROI Project</h4><p>​    就是对原图输入到CNN中去计算，Fast-RCNN的工具包提供提供了3种CNN的结构，默认是使用VGG-16作为CNN的basic structure。根据VGG-16的结构，Fast-RCNN只用了4个MaxPooling层，最后一个换成了ROI Pooling，因此，只需要对Region Proposal的在原图上的4元坐标（x, y, w, h) 除以16，并找到最近的整数，便是ROI Project在feature map上映射的坐标结果。最终得到2k个ROI。</p>
<h4 id="3）ROI-Pooling"><a href="#3）ROI-Pooling" class="headerlink" title="3）ROI Pooling"></a>3）ROI Pooling</h4><p>​    对每一个ROI在feature map上截取后，进行ROI Pooling，就是将每个ROI截取出的块，通过MaxPooling池化到相同维度。</p>
<h4 id="4）FC-Layers-amp-Outputs"><a href="#4）FC-Layers-amp-Outputs" class="headerlink" title="4）FC Layers &amp; Outputs"></a>4）FC Layers &amp; Outputs</h4><p>​    将每个ROI Pooling后的块，通过全连接层生成ROI特征向量，最后用一个Softmax和一个bbox regressor进行分类和回归预测，得到每个ROI的类别分数和bbox坐标。FC Layer运行消耗较多，速度较慢，作者在这里使用了SVD矩阵分解来加快了FC Layer的计算。</p>
<h4 id="5）Multi-task-Loss"><a href="#5）Multi-task-Loss" class="headerlink" title="5）Multi-task Loss"></a>5）Multi-task Loss</h4><p>​    Fast-RCNN的两个任务，一个是分类，分为 n（种类） + 1（背景）类，使用的是Cross Entropy + Softmax的损失函数；第二个是Bbox的Localization回归，使用跟RCNN一样的基于Offset的回归，损失函数使用的是SmoothL1。</p>
<h3 id="2-Fast-RCNN的问题"><a href="#2-Fast-RCNN的问题" class="headerlink" title="2. Fast-RCNN的问题"></a>2. Fast-RCNN的问题</h3><h4 id="1）ROI-Pooling"><a href="#1）ROI-Pooling" class="headerlink" title="1）ROI Pooling"></a>1）ROI Pooling</h4><p>​    在ROI Project中，涉及到region proposal的坐标映射变换问题，在这过程中难免会产生小数坐标。但是在feature map中的点相当于一个个的pixel，是不存在小数的，因此会将小数坐标量化成一个最近的整数，这就会造成一定的误差。在ROI Pooling中，又会有一次的量化。这样，像素坐标经过两次量化后，即使在feature map上有不到1pixel的误差，映射回原图后的误差可能会大于10pixel，这对小物体的检测非常不友好。</p>
<p>​    解决方法：</p>
<ul>
<li><a target="_blank" rel="noopener" href="https://arxiv.org/abs/1703.06870">ROI Align</a>  [2017, Kaiming, Mask-RCNN]</li>
<li><a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1807.11590.pdf">Precise ROI Pooling</a> [2018, IoU-Net]</li>
</ul>
<h4 id="2）"><a href="#2）" class="headerlink" title="2）"></a>2）</h4><h2 id="三-Faster-RCNN-2016-Ren"><a href="#三-Faster-RCNN-2016-Ren" class="headerlink" title="三. Faster-RCNN [2016, Ren]"></a>三. <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1506.01497.pdf">Faster-RCNN</a> [2016, Ren]</h2><h3 id="1-Faster-RCNN算法原理流程"><a href="#1-Faster-RCNN算法原理流程" class="headerlink" title="1.Faster-RCNN算法原理流程"></a>1.Faster-RCNN算法原理流程</h3><p>​    Faster-RCNN在Fast-RCNN的基础上做了两个重大的创新改进，一是在Region Proposal阶段提出了RPN（Region Proposal Network）来代替了Selective Search；二是使用到了Anchor。</p>
<p>​    Faster-RCNN的算法结构如下图所示，可分为Backbone、RPN、Fast-RCNN三个部分。</p>
<p><img src=".%5Cpics%5CFaster-rcnn.png"></p>
<h4 id="1）Backbone"><a href="#1）Backbone" class="headerlink" title="1）Backbone"></a>1）Backbone</h4><p>​    Backbone的作用就是使用CNN对图片进行特征提取，有别于Fast-RCNN的是Faster-RCNN提供的Backbone有ZFNet/ResNet/VGG-16。现在主流都是用ResNet作为Backbone，不过这里作者使用VGG-16（13 conv + 13 ReLU + 4 Pooling）作为演示。输出的feature map维度为[1 x 256 x 38 x 50], 分别代表着[Batch x channel x H/16 x W/16]，其中H，W为输入网络时图片的高和宽，默认H为608，W为800。</p>
<p><img src=".%5Cpics%5Cbackbone.png"></p>
<h4 id="2）RPN"><a href="#2）RPN" class="headerlink" title="2）RPN"></a>2）RPN</h4><p>​    RPN是一个全卷积的神经网络，它的工作原理可以分成三个部分：classification，regression和 proposal；</p>
<p><img src=".%5Cpics%5CRPN.png"></p>
<h5 id="a-Classification"><a href="#a-Classification" class="headerlink" title="a. Classification"></a>a. Classification</h5><p>​    Classification部分是上图中间这条路线，将得到的feature map通过一个3x3和1x1的卷积后，输出的维度为[1 x 18 x 38 x 50]，这18个channel可以分解成2x9，2代表着是否是物体的0/1的score，9代表着9个anchors。因此，特征图维度38x50的每一个点都会生成9个anchor，每个anchor还会有0/1的score。</p>
<p>​    Anchor是图像检测领域一个常用的结构，它可以用来表示原图中物体所在的区域，是一个以feature map上某个点为中心的矩形框。Faster-RCNN的anchor，在feature map上每个点，生成3种尺度和3种比例共9个anchor。下图是一个anchor的示意图，每个点会生成尺度为小(128×128）、中（256×256）、大（512×512），如图中红、绿、蓝色的anchor，1:1, 2:1, 1:2三种比例共9个anchor。这样充分考虑了被检测物体的大小和形状，保证物体都能由anchor生成region proposal。</p>
<img src="F:\blog\CV\RCNN系列\pics\anchor.png" style="zoom:50%;" />

<p>​    卷积完之后，会经过一层reshape将[1 x 18 x 38 x 50]的维度转成 [9 x 2 x 38 x 50]，这是为了方便Softmax做前景/背景的分类运算，如下图所示：</p>
<img src="F:\blog\CV\RCNN系列\pics\reshape1.png" style="zoom: 33%;" />

<p>​    在softmax这层，一方面所有的anchor会与ground truth做IoU运算，将IoU值作为Loss反向传播回去；另一方面对每个anchor进行分类并得到分数，用分数来选择proposal。Softmax结束后，又会有一层reshape，将数据维度转回成原来的[1 x 18 x 38 x 50]。</p>
<h5 id="b-regression"><a href="#b-regression" class="headerlink" title="b. regression"></a>b. regression</h5><p>​    Regression部分是RPN图中的下面那一条分支，原理和Classification部分差不多，feature map通过一个3x3和1x1的卷积后，输出的维度为[1 x 36 x 38 x 50]，其中36个channel可以分成9x4，9就是跟cls部分一样的9个anchor，4是网络根据anchor生成的bbox的4元坐标target的offset。通过offset做bbox regression，再通过下图公式的计算，算出预测bbox的4元坐标（x, y, w, h）来生成region proposal。</p>
<img src="F:\blog\CV\RCNN系列\pics\Bbox-regression.png"/>

<p>​    在这里 Bbox Regression采用损失函数是Smooth L1Loss，本质上就是L1 Loss 和 L2 Loss的结合。对比于L1 Loss 和 L2 Loss，Smooth L1 Loss可以从两方面限制梯度：(1) 当预测框与ground truth的Loss很大的时候，梯度不至于像L2 Loss 那样过大；(2) 当预测框与ground truth的Loss较小的时候，梯度值比L1 Loss更小，不至于跳出局部最优解。</p>
<p><img src="F:\blog\CV\RCNN系列\pics\smoothL1.png"></p>
<img src="F:\blog\CV\RCNN系列\pics\SmoothL1.jpg" style="zoom: 80%;" />

<h5 id="c-Proposal"><a href="#c-Proposal" class="headerlink" title="c. Proposal"></a>c. Proposal</h5><p>​    将<strong>a</strong>, <strong>b</strong> 部分的结果综合计算，便可以得出Region Proposal。若anchor的IoU &gt; 0.7，就认为是前景，若 IoU &lt; 0.3，就认为是背景，其他的anchor全都忽略。一般来说，前景和背景的anchor保留的比例为1:3 。</p>
<h4 id="3）ROI-Fast-RCNN"><a href="#3）ROI-Fast-RCNN" class="headerlink" title="3）ROI + Fast-RCNN"></a>3）ROI + Fast-RCNN</h4><p>​    之后的部分就跟Fast RCNN一样的操作了，进行ROI Pooling 后进行cls 和 reg。</p>
<h3 id="2-Faster-RCNN训练方法"><a href="#2-Faster-RCNN训练方法" class="headerlink" title="2. Faster-RCNN训练方法"></a>2. Faster-RCNN训练方法</h3><h4 id="1）经典的训练方法"><a href="#1）经典的训练方法" class="headerlink" title="1）经典的训练方法"></a>1）经典的训练方法</h4><p>​    经典的Faster-RCNN训练方法可分为四步：</p>
<ul>
<li><ol>
<li>使用在 ImageNet pretrained的参数模型来 Fine-tune 训练 Backbone 和 RPN；</li>
</ol>
</li>
<li><ol start="2">
<li>使用第1步训练后生成的region proposal，来训练 Fast-RCNN;</li>
</ol>
</li>
<li><ol start="3">
<li>使用第2步生成的检测结果去初始化训练RPN网络，Backbone不参与训练；</li>
</ol>
</li>
<li><ol start="4">
<li>使用第3步训练后的RPN在再生成region proposal，用来Fine-tune Fast-RCNN；</li>
</ol>
</li>
</ul>
<h4 id="2）其他方法"><a href="#2）其他方法" class="headerlink" title="2）其他方法"></a>2）其他方法</h4><p>​    其他方法就是…直接把Faster-RCNN当一个整体，做一个end-to-end的一步训练。</p>
<h3 id="3-相关源码"><a href="#3-相关源码" class="headerlink" title="3.相关源码"></a>3.相关源码</h3>
      
    </div>
    <footer class="article-footer">
      <a data-url="http://example.com/2021/04/29/CV/RCNN-family/RCNN%EF%BC%8CFast%20RCNN%EF%BC%8CFaster%20RCNN%E7%AE%97%E6%B3%95%E5%8E%9F%E7%90%86/" data-id="cko2b5ema000b2wvca92t378l" data-title="" class="article-share-link">Teilen</a>
      
      
      
    </footer>
  </div>
  
</article>



  
    <article id="post-CV/facebagnet&amp;attention/SENet" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2021/04/29/CV/facebagnet&attention/SENet/" class="article-date">
  <time class="dt-published" datetime="2021-04-29T03:06:22.866Z" itemprop="datePublished">2021-04-29</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <h1 id="SENet"><a href="#SENet" class="headerlink" title="SENet"></a>SENet</h1><h2 id="如何生成attention权重矩阵"><a href="#如何生成attention权重矩阵" class="headerlink" title="如何生成attention权重矩阵"></a>如何生成attention权重矩阵</h2><p>我们可以通过神经网络生成，这是因为：每个channel的重要程度与最终的任务决定的，而任务与loss有关，所以权重可以在训练过程中生成。因此在训练过程中，每个feature map 和它在最终任务中的权重可以由一个神经网络建立函数关系，便可以生成其权重。</p>
<p>于是我们的任务转到如何设计这个神经网络。</p>
<p>最常用的思路是对feature map每个channel进行global pooling -&gt; FC1 -&gt; ReLU -&gt; FC2 -&gt; Sigmoid，其中FC1和ReLU的维度是1x1x(C/r)，其他层的维度都是1x1xC。这样一个类似于bottleneck的结构被实验认为在生成attention权重中是有效的。这就是SENet（Squeeze-and-Excitation Networks）的模型结构。</p>
<img src="./img/image-20210223142843988.png" alt="image-20210223142843988" style="zoom:25%;" />

<h2 id="SE-ResNet"><a href="#SE-ResNet" class="headerlink" title="SE-ResNet"></a>SE-ResNet</h2><p>SE-ResNet，就是SENet在ResNet上的应用，如下图所示，在Residual模块的基础上再加上了SENet的模块。通过SENet计算每个channel的权重后，再与原来的每个channel相乘，这样就得到了SE-ResNet的整个模块。</p>
<img src=".\img\image-20210223143738317.png" alt="image-20210223143738317" style="zoom:33%;" />

<h1 id="余弦周期学习率退火使用方法"><a href="#余弦周期学习率退火使用方法" class="headerlink" title="余弦周期学习率退火使用方法"></a>余弦周期学习率退火使用方法</h1><p>训练一次得到m个模型，m个模型的保存时机：a. 学习率每变化一个周期；b. 验证集的loss，acc准确率在周期内效果最好。</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://example.com/2021/04/29/CV/facebagnet&attention/SENet/" data-id="cko2b5eme000f2wvc8tsgb4da" data-title="" class="article-share-link">Teilen</a>
      
      
      
    </footer>
  </div>
  
</article>



  
    <article id="post-CV/facebagnet&amp;attention/FaceBagNet" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2021/04/29/CV/facebagnet&attention/FaceBagNet/" class="article-date">
  <time class="dt-published" datetime="2021-04-29T03:06:22.835Z" itemprop="datePublished">2021-04-29</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <h1 id="FaceBagNet"><a href="#FaceBagNet" class="headerlink" title="FaceBagNet"></a>FaceBagNet</h1><h2 id="三元子的筛选"><a href="#三元子的筛选" class="headerlink" title="三元子的筛选"></a>三元子的筛选</h2><ol>
<li>随机，但是随机筛选的不好的话训练就会失败。</li>
<li>Anchor随机，再在所有样本中选一个其对应的Positive和Negative。（最好的，但是比较难训练）</li>
<li>在minibatch中随机筛选，每个样本选40个positive，且选择的negative与anchor的距离要大于positive与anchor的距离。</li>
</ol>
<h2 id="Adagrad优化方法"><a href="#Adagrad优化方法" class="headerlink" title="Adagrad优化方法"></a>Adagrad优化方法</h2><p>a = 0.2</p>
<h2 id="数据集"><a href="#数据集" class="headerlink" title="数据集"></a>数据集</h2><h2 id="Embedding"><a href="#Embedding" class="headerlink" title="Embedding"></a>Embedding</h2><p>实验embedding的维度64/128/256/512，发现128维更好。</p>
<h2 id="VGGFACE训练方法"><a href="#VGGFACE训练方法" class="headerlink" title="VGGFACE训练方法"></a>VGGFACE训练方法</h2><p>先分类，再finetune学习映射层</p>
<p>方法步骤：1. VGGNet+Softmax Loss，分2622维度（VGGFace数据集维度）</p>
<pre><code>                2. VGGNet+Triplet Loss，1024维embedding
</code></pre>
<p>VGGFace数据集维度：2622人，每人1000张，260w张人脸。</p>
<p>对比方式：欧氏距离</p>
<p>新概念：度量学习（metric learning）学习独特性和紧凑性</p>
<p>tips：训练数据一定的数据错误，对模型有益。</p>
<p>与FaceNet相比，VGGFace用较少的数据就能达到LFW较高的准确率98.95%。</p>
<h2 id="Center-Loss"><a href="#Center-Loss" class="headerlink" title="Center Loss"></a>Center Loss</h2>
      
    </div>
    <footer class="article-footer">
      <a data-url="http://example.com/2021/04/29/CV/facebagnet&attention/FaceBagNet/" data-id="cko2b5emb000d2wvc62468fpy" data-title="" class="article-share-link">Teilen</a>
      
      
      
    </footer>
  </div>
  
</article>



  
    <article id="post-CV/facebagnet&amp;attention/attention" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2021/04/29/CV/facebagnet&attention/attention/" class="article-date">
  <time class="dt-published" datetime="2021-04-29T03:06:22.821Z" itemprop="datePublished">2021-04-29</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <h1 id="遮挡人脸识别的Attention机制"><a href="#遮挡人脸识别的Attention机制" class="headerlink" title="遮挡人脸识别的Attention机制"></a>遮挡人脸识别的Attention机制</h1><h2 id="数据获取"><a href="#数据获取" class="headerlink" title="数据获取"></a>数据获取</h2><ol>
<li>生成；</li>
<li>真实数据</li>
</ol>
<h2 id="相关attention模型"><a href="#相关attention模型" class="headerlink" title="相关attention模型"></a>相关attention模型</h2><h3 id="1-SENet"><a href="#1-SENet" class="headerlink" title="1. SENet"></a>1. SENet</h3><h4 id="如何生成attention权重矩阵"><a href="#如何生成attention权重矩阵" class="headerlink" title="如何生成attention权重矩阵"></a>如何生成attention权重矩阵</h4><p>我们可以通过神经网络生成，这是因为：每个channel的重要程度与最终的任务决定的，而任务与loss有关，所以权重可以在训练过程中生成。因此在训练过程中，每个feature map 和它在最终任务中的权重可以由一个神经网络建立函数关系，便可以生成其权重。</p>
<p>于是我们的任务转到如何设计这个神经网络。</p>
<p>最常用的思路是对feature map每个channel进行global pooling -&gt; FC1 -&gt; ReLU -&gt; FC2 -&gt; Sigmoid，其中FC1和ReLU的维度是1x1x(C/r)，其他层的维度都是1x1xC。这样一个类似于bottleneck的结构被实验认为在生成attention权重中是有效的。这就是SENet（Squeeze-and-Excitation Networks）的模型结构。</p>
<img src="F:/blog/CV/facebagnet&attention/img/image-20210223142843988.png" alt="image-20210223142843988" style="zoom:25%;" />

<h4 id="SE-ResNet"><a href="#SE-ResNet" class="headerlink" title="SE-ResNet"></a>SE-ResNet</h4><p>SE-ResNet，就是SENet在ResNet上的应用，如下图所示，在Residual模块的基础上再加上了SENet的模块。通过SENet计算每个channel的权重后，再与原来的每个channel相乘，这样就得到了SE-ResNet的整个模块。</p>
<img src="F:/blog/CV/facebagnet&attention/img/image-20210223143738317.png" alt="image-20210223143738317" style="zoom:33%;" />

<h4 id="SENet的改进思路"><a href="#SENet的改进思路" class="headerlink" title="SENet的改进思路"></a>SENet的改进思路</h4><p>​    后来有人做实验为了改进SENet，增加MaxPooling，去掉ReLU，实验得到的效果更好了，由这个思路得到了的CBAM。</p>
<h3 id="2-CBAM-——-Convolutional-Block-Attention-Module"><a href="#2-CBAM-——-Convolutional-Block-Attention-Module" class="headerlink" title="2. CBAM —— Convolutional Block Attention Module"></a>2. CBAM —— Convolutional Block Attention Module</h3><p>CBAM的结构如图所示，包含Channel Attention和Spatial Attention两个模块，这两个模块的计算原理是先通过输入的特征图提取Channel Attention(Mc)，再用相乘后的Feature提取Spatial Attention(Ms)，相乘后就得到了包含注意力的Refined Feature Map 。Input Feature Map的维度是512x512x64。</p>
<p><img src="F:\blog\CV\facebagnet&attention\img\cbam.png"></p>
<p>Channel Attention Module的具体结构如下，输入的feature map以channel为维度分别进行MaxPooling和AVGPooling，最后得到两个64维的feature 向量后经过MLP，最后将两个向量进行相加融合后经过Sigmoid得到channel attention，即Mc。其中Shared MLP 为64-&gt;32-&gt;64的FC层。</p>
<p><img src="F:\blog\CV\facebagnet&attention\img\cam.png"></p>
<p>其计算公式如下：</p>
<p><img src="F:\blog\CV\facebagnet&attention\img\formula.png"></p>
<p>Spatial Attention Module的具体结构如下，与上面的类似，输入的feature map以每个空间点为维度分别进行MaxPooling和AVGPooling，输出两个一个512x512的attention map，对两个attention map进行concat后，然后通过7x7的卷积核进行特征融合，最后通过Sigmoid得到一个跟feature map相同大小的Attention map，即Ms。</p>
<img src="F:\blog\CV\facebagnet&attention\img\sam.png" style="zoom: 33%;" />

<p>其计算公式如下：</p>
<img src="F:\blog\CV\facebagnet&attention\img\formula2.png" style="zoom: 33%;" />

<p>为什么不是先Spatial再Channel？为什么不同时得到Mc和Ms？作者做实验得到的结果显示，这样效果最好，理论没法解释…这也是一个后续研究的方向。</p>
<p>将CBAM与ResBlock相结合的结构如下图，在每层卷积的后面加上CBAM。</p>
<p><img src="F:\blog\CV\facebagnet&attention\img\ResBlock+CBAM.png"></p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://example.com/2021/04/29/CV/facebagnet&attention/attention/" data-id="cko2b5emd000e2wvcctt4fz2k" data-title="" class="article-share-link">Teilen</a>
      
      
      
    </footer>
  </div>
  
</article>



  
    <article id="post-CV/face-recognize/SDK" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2021/04/29/CV/face-recognize/SDK/" class="article-date">
  <time class="dt-published" datetime="2021-04-29T03:06:22.713Z" itemprop="datePublished">2021-04-29</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <h1 id="SDK"><a href="#SDK" class="headerlink" title="SDK"></a>SDK</h1><p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/54665674">https://zhuanlan.zhihu.com/p/54665674</a></p>
<h2 id="一般常用的落地方法"><a href="#一般常用的落地方法" class="headerlink" title="一般常用的落地方法"></a>一般常用的落地方法</h2><p>互联网：Webserver方式落地</p>
<p>垂直行业：Java/C/C++、Android/IOS  SDK方式落地</p>
<p>pytorch多数情况下不能直接使用</p>
<h2 id="Webserver方式"><a href="#Webserver方式" class="headerlink" title="Webserver方式"></a>Webserver方式</h2><p>目标/难点：如何把模型的inference部分放在后端？</p>
<p>常用的后端：</p>
<p>Python：Django、Flask</p>
<p>PHP：调用Python方法或者重写</p>
<p>JSP</p>
<p>部署方式：</p>
<p>a. NCNN、TVM、ONNX、TFlite、CoreML，速度快，但层的支持更新较慢;</p>
<p>b. Torch Script，速度慢。</p>
<p>某些平台支持全自动发布服务。</p>
<h4 id="PyTorch-Flask-API"><a href="#PyTorch-Flask-API" class="headerlink" title="PyTorch Flask API"></a>PyTorch Flask API</h4><p>做demo推荐</p>
<h4 id="Django"><a href="#Django" class="headerlink" title="Django"></a>Django</h4><p><a target="_blank" rel="noopener" href="https://www.djangoproject.com/">https://www.djangoproject.com/</a></p>
<h2 id="落地到嵌入式设备（手机、开发板）"><a href="#落地到嵌入式设备（手机、开发板）" class="headerlink" title="落地到嵌入式设备（手机、开发板）"></a>落地到嵌入式设备（手机、开发板）</h2><p>YOLO V5部署到手机：IDetection。</p>
<p>PyTorch部署到安卓 git clone <a target="_blank" rel="noopener" href="https://githun.com/pytorch/android-dem-app.git">https://githun.com/pytorch/android-dem-app.git</a></p>
<p>PyTorch部署到安卓 pytorch.org/mobile/ios/</p>
<h2 id="把自己的模型封装到PyPI"><a href="#把自己的模型封装到PyPI" class="headerlink" title="把自己的模型封装到PyPI"></a>把自己的模型封装到PyPI</h2><h2 id="PHP如何调用Python"><a href="#PHP如何调用Python" class="headerlink" title="PHP如何调用Python"></a>PHP如何调用Python</h2><p>调用 command 命令 巡行python 将返回值存为变量。</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://example.com/2021/04/29/CV/face-recognize/SDK/" data-id="cko2b5ema000c2wvce4zb4wlg" data-title="" class="article-share-link">Teilen</a>
      
      
      
    </footer>
  </div>
  
</article>



  
    <article id="post-CV/face-recognize/Face Recognize" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2021/04/29/CV/face-recognize/Face%20Recognize/" class="article-date">
  <time class="dt-published" datetime="2021-04-29T03:06:22.670Z" itemprop="datePublished">2021-04-29</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <h1 id="Face-Recognize综述"><a href="#Face-Recognize综述" class="headerlink" title="Face Recognize综述"></a>Face Recognize综述</h1><h2 id="如何在人群中找到“张三”"><a href="#如何在人群中找到“张三”" class="headerlink" title="如何在人群中找到“张三”"></a>如何在人群中找到“张三”</h2><p>​    做个二分类器 ”张三“ VS ”其他人“</p>
<p>​    如何训练？ 样本如何组织。样本一：张三、样本二：非张三…</p>
<p>​    这个方法对单人的人脸检测可以用，但是如果要训练一个model来实现多人的人脸识别，则应该使用多分类的方法—识别多少个人就训练多少个分类。</p>
<p>​    但是多分类也只适用于小规模的人群，人的数量一旦变化，这个模型就不一定可用了。</p>
<p>​    如何找到任意人？ 如果用分类的思路，就只能用多分类，而分类数是有界限的，比如1000~2000个人。可是在用户数足够多的情况下，比如1w人，10w人，多分类是达不到大规模识别的要求。</p>
<p>​    AlexNet的作者提到，CNN的分类特征具有各种“不变性”：光照不变性、旋转不变性、尺度不变性、姿态不变性等理想特性。因此可以用CNN的分类特征代表人脸，即Face ID/Face DNA。</p>
<h2 id="FaceNet-——-构造Face-ID"><a href="#FaceNet-——-构造Face-ID" class="headerlink" title="FaceNet —— 构造Face ID"></a><a href="./docs/facenet.pdf">FaceNet</a> —— 构造Face ID</h2><p>​    我们希望构建一个网络，输入是m x n维的的人脸matrix，输出是一个1 x d维的vector作为Face ID。方法可以是首先构造多分类器，取fc层的feature maps作为Face ID，这样Face ID的性能将由数据和Loss Function决定。数据是足够的，那提高性能就落在了Loss Function身上。</p>
<p>​    如何提升Loss Function，之前做分类常用的是softmax + mse或者softmax + crossentropy进行训练，但是Face ID的特点就是类间特征距离很近，如果两个人脸长得很像容易误判，于是需要改进Loss Function来弥补这一问题。</p>
<p>​    构造改进的Loss应该满足一下条件：1.用Loss求得的类内距离Ds，应该小于类间距离Dns，即Ds &lt; Dns；2. Ds要尽量小，取值为[0, 1]，Dns要尽量大 ，取值为[0, 1]，因此Ds - Dns的取值为[-1, 1]；3. Loss应该以0为目标，为了防止梯度爆炸 ，且模型权重与feature map都能符合0均值分布。</p>
<p>​    用Ds - Dns + 1作为Loss理论上是可行的，但是在实际实验过程中会发现+1训练时会出现许多的问题，难以训练。所以一般采用Max（0，Ds - Dns + 0.5）作为Loss的形式。根据以上条件，Ds &lt; Dns - 0.5，这也能解释为什么Ds - Dns + 1作为Loss比较难以训练，因为根据条件Ds &lt; Dns - 1，而Dns取值为[0, 1]，则要求Ds小于0与之前的条件矛盾，因此难以训练。 </p>
<p>​    通常用3个样本便可以求出Ds和Dns，参考样本anchor、positive和negative样本。D是可以通过anchor与positive样本求出，Dns则通过anchor与negative样本求出。这样便可以构成triplet loss，也叫三元子或三联子Loss，公式为Loss = Max（0，Ds - Dns + a）。</p>
<p>​    如何构造Dataset进行训练？要计算一个triplet loss需要3个样本，</p>
<p>且样本要满足一个anchor样本，一个positive样本和一个negative样本。得到三个样本后，这三个样本都要依次经过facenet得到三个样本的face embedding特征，然后将三个embedding通过公式计算triplet loss。</p>
<p>​    常用的公开数据集有LFW和VGGFace2。不过LFW好像已经被刷爆了…</p>
<p>​    评价指标</p>
<p>​    等误率（EER），就是两个相等的错误率，这两个错误率分别是：1. 把本不是同一个人的脸识别为同一个人人脸；2. 把本是同一个人的脸识别为不同人脸。当将模型输出的人脸特征通过相似度计算后，设定的阈值会对这两个错误率有影响。</p>
<p>​    ROC曲线</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://example.com/2021/04/29/CV/face-recognize/Face%20Recognize/" data-id="cko2b5em8000a2wvcfcbm7jaq" data-title="" class="article-share-link">Teilen</a>
      
      
      
    </footer>
  </div>
  
</article>



  
    <article id="post-CV/face-recognize/FACE EMBEDDING" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2021/04/29/CV/face-recognize/FACE%20EMBEDDING/" class="article-date">
  <time class="dt-published" datetime="2021-04-29T03:06:22.449Z" itemprop="datePublished">2021-04-29</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <h1 id="FACE-EMBEDDING"><a href="#FACE-EMBEDDING" class="headerlink" title="FACE EMBEDDING"></a>FACE EMBEDDING</h1><h2 id="L2-Softmax-Loss"><a href="#L2-Softmax-Loss" class="headerlink" title="L2-Softmax Loss"></a>L2-Softmax Loss</h2><p>Softmax优化的两个维度：a. 特征模值的大小；b. 特征的方向；若能减少一个优化维度，便可以简化优化问题，从而提高模型精度。特征的方向是无法固定的，但是模值大小是可以固定的，这就是L2-norm Softmax的思想。</p>
<p>一般大小a固定为16，实验得出的效果最好。</p>
<h2 id="三元子的筛选"><a href="#三元子的筛选" class="headerlink" title="三元子的筛选"></a>三元子的筛选</h2><ol>
<li>随机，但是随机筛选的不好的话训练就会失败。</li>
<li>Anchor随机，再在所有样本中选一个其对应的Positive和Negative。（最好的，但是比较难训练）</li>
<li>在minibatch中随机筛选，每个样本选40个positive，且选择的negative与anchor的距离要大于positive与anchor的距离。</li>
</ol>
<h2 id="Adagrad-优化器"><a href="#Adagrad-优化器" class="headerlink" title="Adagrad 优化器"></a>Adagrad 优化器</h2><p>超参数 a = 0.2</p>
<h2 id="FaceNet所用的数据集"><a href="#FaceNet所用的数据集" class="headerlink" title="FaceNet所用的数据集"></a>FaceNet所用的数据集</h2><p>8 million人，200million图片，100million图片测试，在LFW中测试成绩99.63%</p>
<h2 id="embedding维度"><a href="#embedding维度" class="headerlink" title="embedding维度"></a>embedding维度</h2><p>经过实验embedding的维度，在64/128/256/512维度的embedding中128维的效果最好。</p>
<p>不需要对齐，只需要脸部裁剪。</p>
<h1 id="VGGFace"><a href="#VGGFace" class="headerlink" title="VGGFace"></a>VGGFace</h1><p>训练方法： 非 end-to-end</p>
<p>构建最少的人为干预大规模人脸数据集，2622人，每人1000张，260万个人脸。</p>
<p>分类器方法：先用VGGNet + Softmax Loss，2622类（数据集维度）</p>
<p>​                        再用VGGNet + Triplet Loss，1024维。</p>
<p>tips：先分类，再Fine-tune学习映射层，效果才更好；</p>
<p>​            一定程度的训练数据错误对模型是有益的；</p>
<p>人脸验证对比方法：欧氏距离</p>
<p>新概念：度量学习（metric learning）用于学习独特性和紧凑性</p>
<h2 id="Center-Loss"><a href="#Center-Loss" class="headerlink" title="Center Loss"></a>Center Loss</h2><p> 辅助Softmax Loss进行人脸识别的训练，Loss = L_Softmax + λ · L_center ，λ为Center Loss的权重。Center Loss的就是当物体与当前所属类别中心的欧氏距离。通过网络最后输出的feature embedding向量（128dim）与所有样本中每个类别中，在当前网络情况下计算出的中心点进行欧氏距离的计算，因此中心点会随着模型训练动态变化，也就是随着weights漂移。中心点C可以通过梯度进行计算</p>
<h2 id="Cos-Face"><a href="#Cos-Face" class="headerlink" title="Cos Face"></a>Cos Face</h2><p>Large Margin Cosine Loss Deep Face。 在loss公式中的cos加上一个margin，也就是在正负样本的中间地带加了margin，使得正负样本学习得更具区分性。</p>
<h2 id="Arc-face"><a href="#Arc-face" class="headerlink" title="Arc face"></a>Arc face</h2><p>Arc face与cos face的原理类似，就是在loss公式中cos的角度θ中加上margin，训练出的效果比cos face更优，不需要与其他Loss一起训练。</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://example.com/2021/04/29/CV/face-recognize/FACE%20EMBEDDING/" data-id="cko2b5em800092wvcayabh3va" data-title="" class="article-share-link">Teilen</a>
      
      
      
    </footer>
  </div>
  
</article>



  


  <nav id="page-nav">
    
    <span class="page-number current">1</span><a class="page-number" href="/page/2/">2</a><a class="extend next" rel="next" href="/page/2/">weiter &raquo;</a>
  </nav>

</section>
        
          <aside id="sidebar">
  
    

  
    

  
    
  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archiv</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2021/04/">April 2021</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">letzter Beitrag</h3>
    <div class="widget">
      <ul>
        
          <li>
            <a href="/2021/04/29/CV/YOLO/Yolo%E7%B3%BB%E5%88%97%E6%A3%80%E6%B5%8B%E7%AE%97%E6%B3%95%E5%8E%9F%E7%90%86/">(no title)</a>
          </li>
        
          <li>
            <a href="/2021/04/29/CV/train/model-train/">(no title)</a>
          </li>
        
          <li>
            <a href="/2021/04/29/CV/train/data-preprocessing/">(no title)</a>
          </li>
        
          <li>
            <a href="/2021/04/29/CV/RCNN-family/RCNN%EF%BC%8CFast%20RCNN%EF%BC%8CFaster%20RCNN%E7%AE%97%E6%B3%95%E5%8E%9F%E7%90%86/">(no title)</a>
          </li>
        
          <li>
            <a href="/2021/04/29/CV/facebagnet&attention/SENet/">(no title)</a>
          </li>
        
      </ul>
    </div>
  </div>

  
</aside>
        
      </div>
      <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      
      &copy; 2021 Cheng Zixu<br>
      Powered by <a href="https://hexo.io/" target="_blank">Hexo</a>
    </div>
  </div>
</footer>

    </div>
    <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>
    


<script src="/js/jquery-3.4.1.min.js"></script>



  
<script src="/fancybox/jquery.fancybox.min.js"></script>




<script src="/js/script.js"></script>





  </div>
</body>
</html>